{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import time\n",
    "from typing import List\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.dates as mdates\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.cluster import MeanShift, KMeans\n",
    "from IPython.core.debugger import set_trace\n",
    "from matplotlib import pyplot, dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need data to analyze. From our milestone we can infer that we will need to be able to get pageviews from both specific articles on Wikipedia, as well as the aggregated pageviews for an entire language project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load our generated list of articles\n",
    "For our analysis we have aggregated a set of articles representing \"Internet software privacy\" that we need to analyze. We start out by loading that list of articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load article titles\n",
    "english_names = pd.read_csv('data/articles.csv', header=None, names=['Title'])\n",
    "\n",
    "# transform article names so that we can easily query them in the APIs\n",
    "english_names['Title'] = english_names['Title'].apply(lambda x : x.replace(' ', '_'))\n",
    "english_names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve wikidata entries for each given article\n",
    "Now we proceed by getting the wikidata IDs for each article in our list. We need these so that we can retrieve the equivalent articles in other language projects of Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qid_from_title(title: str, language: str) -> str:\n",
    "    \"\"\"\n",
    "    Gets the Wikidata ID for the given article on the given language project.\n",
    "    \n",
    "    :param title: the article title to retrieve qid for\n",
    "    :param language: the string representing the language project the article title comes from\n",
    "    \n",
    "    :returns: the Wikidata ID for the article\n",
    "    \"\"\"\n",
    "    response = requests.get(f'https://{language}.wikipedia.org/w/api.php?'\n",
    "                            f'action=query&prop=pageprops&titles={title}&redirects&format=json')\n",
    "    try:\n",
    "        r = [item for item in response.json()['query']['pages'].values()][0]\n",
    "        qid = r['pageprops']['wikibase_item']\n",
    "    except KeyError:\n",
    "        print(f'Article {title} has no Wikidata ID')\n",
    "        return None\n",
    "    return qid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve Wikidata IDs\n",
    "qids = pd.DataFrame([get_qid_from_title(title, 'en') for title in tqdm(english_names.Title.values)])\n",
    "qids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve English pageview data \n",
    "We now utilize https://www.wikishark.com/ to retrieve the actual page view data for each of the articles on the English language Wikipedia project. We have contacted the developer of this website to grant us the permission to call their backends that we found by doing some 1337 hacking. We are allowed to do so for the \"Internet privacy software\"-related articles, if we wait 1-2 seconds in between each request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_year_to_date(year: int, days: int) -> datetime.datetime:\n",
    "    \"\"\"\n",
    "    Takes the given numerical year and days passed of that year \n",
    "    and returns the datetime object representing that date.\n",
    "    \n",
    "    :param year: the christian calendar year representing the date wanted\n",
    "    :param days: the amount of days passed in that year\n",
    "    \n",
    "    :returns: datetime object corresponding to the specified date\n",
    "    \"\"\"\n",
    "    return datetime.datetime(year, 1, 1) + datetime.timedelta(days - 1)\n",
    "\n",
    "def get_wikishark_id(article_name: str, language: str) -> str:\n",
    "    \"\"\"\n",
    "    Gives the wikishark internal ID for the given article and language project.\n",
    "    \n",
    "    :param article_name: the article title for which the internal ID is requested.\n",
    "    :param language: the string representing the language project the article title comes from\n",
    "    \n",
    "    :returns: string corresponding to the retrieved wikishark ID for the title\n",
    "    \"\"\"\n",
    "    response = requests.get(f'https://www.wikishark.com/autocomplete.php?q={article_name}')\n",
    "    r = response.json()\n",
    "    \n",
    "    target = None\n",
    "    for candidate in r:\n",
    "        if '(' + language + ')' in candidate['name'] and article_name.replace('_',' ').lower() in candidate['name'].lower():\n",
    "            return candidate['id']\n",
    "    return target\n",
    "\n",
    "def get_daily_pageviews(titles: List[str], language: str, start: str, end: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves the daily page views for the given articles on the given language edition of Wikipedia.\n",
    "    Filtering is also performed to only return the page views for the given time span.\n",
    "    \n",
    "    :param titles: the list of articles to retrieve page views for\n",
    "    :param language: the string representing the language project the article titlse come from\n",
    "    \n",
    "    :returns: DataFrame containing the daily pageviews for the articles in the time span specified\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for title in tqdm(titles):\n",
    "        \n",
    "        # developer requested we wait 1 second at least in between requests\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # retrieve wikishark ID for article\n",
    "        wikishark_id = get_wikishark_id(title, language)\n",
    "        if wikishark_id is None:\n",
    "            print(f'Could not find data for {title}')\n",
    "            continue\n",
    "        \n",
    "        # wait to avoid overloading servers\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # retrieve page views for given title\n",
    "        response = requests.get(f'https://www.wikishark.com/getdata/daily.php?value={wikishark_id}?view=2&scale=0&normalized=0&loglog=0&log=0&zerofix=0')\n",
    "        daily_data = response.json()\n",
    "        \n",
    "        # add data with timestamps\n",
    "        start_date = datetime.datetime.strptime(start, '%d/%m/%Y')\n",
    "        end_date   = datetime.datetime.strptime(end, '%d/%m/%Y')\n",
    "        current_date = datetime.datetime.now()\n",
    "        \n",
    "        # wikishark returns daily page views for every day since 2007-12-31 (independent of given parameters)\n",
    "        # we need to index it according to the time period we are interested in\n",
    "        start_index = (len(daily_data) - 1) - (current_date - start_date).days\n",
    "        end_index = (len(daily_data) - 1) - (current_date - end_date).days\n",
    "        \n",
    "        # add page views for each day\n",
    "        timestamps = {}\n",
    "        for i, d in enumerate(daily_data[start_index:end_index]):\n",
    "            ts = start_date + datetime.timedelta(days=i)\n",
    "            timestamps[ts] = int(d)\n",
    "    \n",
    "        # add data for given article to collection\n",
    "        data.append({**{'Article': title}, **timestamps})\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next cell we have made it possible to both regenerate the data set yourself, or just load from the pickled object. Given that we have to wait 1 second in between requests to the wikishark API: we would suggest loading the pickled object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PICKLED = './privacy.pkl'\n",
    "\n",
    "# load pickled data if available in root of directory\n",
    "if os.path.isfile(PICKLED):\n",
    "    privacy_en = pd.read_pickle(PICKLED)\n",
    "    \n",
    "# reconstruct data from scratch\n",
    "else:\n",
    "    privacy_en = get_daily_pageviews(list(english_names.Title), 'en', '01/01/2011','01/01/2016')\n",
    "    \n",
    "    # use article name as index\n",
    "    privacy_en.index = privacy_en.Article\n",
    "    privacy_en = privacy_en.drop(['Article'], axis=1)\n",
    "    \n",
    "    # pickle data to avoid having to re-request it\n",
    "    privacy_en.to_pickle(PICKLED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate into monthly data\n",
    "Our data from above comes in a daily resolution. Our analysis will be on a monthly basis so we need to aggregate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make columns datetime objects for resampling to work\n",
    "privacy_en.columns = pd.to_datetime(privacy_en.columns)\n",
    "\n",
    "# take monthly cumulative\n",
    "monthly_en = privacy_en.resample('M', axis=1).sum()\n",
    "monthly_en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis \n",
    "We now perform an exploratory data analysis to identify any issues with the data we've generated so far. This is different from the approach taken in the original paper, where issues with outliers were identified and corrected for in another iteration of the analysis.\n",
    "\n",
    "We start by melting our data so that it is easier to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt dataframe to use dates as entry values rather than columns\n",
    "monthly_en_melt = pd.melt(monthly_en, value_name='views', var_name='date', ignore_index=False)\n",
    "monthly_en_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at the summed data real quick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_en_melt.reset_index().groupby('date').sum().reset_index().plot.scatter(x='date',y='views')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick preprocessing of articles\n",
    "\n",
    "Since we have a lot of articles, we want to first remove articles that are of no interest to us, before we can visualize the total dataset correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all articles that have a very low amount of views no matter what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_views = [article for article in monthly_en_melt.index.unique() if (monthly_en_melt.loc[article].views < 100).all()]\n",
    "print(low_views)\n",
    "monthly_en_melt = monthly_en_melt.drop(low_views)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all articles that may be outliers\n",
    "\n",
    "In the same way the original paper found outlier like the hamas article, we want to remove from our dataset sudden changes in pageviews that are not part of our signal.\n",
    "Since the views an article gets monthly doesn't follow a gaussian distribution, we use IQRs to determine if an article is an outlier rather than just using the 3 standard deviations rule. If the views in a month are higher than the 75th percentile + 3 times the interquartile range, we consider it for manual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_outlier(article, factor=2):\n",
    "    \"\"\"\n",
    "    Computes if the given entry is an outlier article using IQR in our local dataframe.\n",
    "    \"\"\"\n",
    "    q25, q75 = np.percentile(monthly_en_melt.loc[article].views, 25), np.percentile(monthly_en_melt.loc[article].views, 75)\n",
    "    iqr = q75 - q25\n",
    "    cut_off = iqr * factor\n",
    "    lower, upper = q25 - cut_off, q75 + cut_off\n",
    "    return ((monthly_en_melt.loc[article].views < lower) | (monthly_en_melt.loc[article].views > upper)).any()\n",
    "\n",
    "outliers = [article for article in monthly_en_melt.index.unique() if _is_outlier(article)]\n",
    "print(f'Number of outliers identified: {len(outliers)}')\n",
    "print(f'Titles: {outliers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual inspection\n",
    "\n",
    "In order to verify that our potential outliers should be removed, we manually check each of these articles, the news related to it, to find out if there is an obvious reason for the sudden change in views, and if this reason is related to our signal: privacy enhancing technologies.\n",
    "\n",
    "The articles we suspect as outliers are:\n",
    "- **I2P**: Spike happens around the time Silkroad (drug network) moves from tor to I2p\n",
    "- **IMule**: High variance all around, low views, bump in 2012 probably related to the bump in use of i2p in 2012\n",
    "- **Operation onymous**: Spikes around the time of operation, an international law enforcement operation targeting darknet markets and other hidden services operating on the Tor network.\n",
    "- **Tor**: Spike happened when SilkRoad was shut down in october 2013, spike in views was quite large and somewhat unrelated.\n",
    "- **Bitblinder**: Spike happens in  March 2011 when researchers documented an attack that is capable of revealing the IP addresses of BitTorrent users on the Tor network. \n",
    "- **2channel** and **4chan**: Spikes happen in Sep 2015 when the founder of 4chan,  Christopher Poole, formally announced on 21 September 2015 that he had sold the website to the founder of 2channel, \tHiroyuki Nishimura. \n",
    "- **Bitmessage**: Spike happened in Sep 2014 when there were tons of messages being delayed or missing, and also there was word that the network is being attacked by spam. The next update was released then in October 2014 to address these problems.\n",
    "- **Confide**: The pageviews we got from wikishark don't correspond to the article about the Confide app, but another article with the same name (disambiguation is not working very well on wikishark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier removal\n",
    "We remove the aforementioned articles from our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_outliers = ['4chan','I2P', 'IMule', 'Operation_Onymous', 'Tor_(anonymity_network)', '2channel', 'Bitmessage', 'Confide']\n",
    "monthly_en_melt= monthly_en_melt.drop(manual_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally some data visualization\n",
    "We produce a set of plots to get more of a feel of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting per article views (total)\n",
    "This will let us see the page view contribution of each article to the overall signal we are analyzing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort articles by total views in time period\n",
    "most_views = monthly_en_melt.reset_index().groupby('Article')['views'].sum().sort_values(ascending=False)\n",
    "\n",
    "# keep the index for the five articles with the most views\n",
    "head = list(most_views.head(5).index) # a surprise tool that will help us later\n",
    "\n",
    "# plot the cumulative views for the articles\n",
    "plt.figure(figsize=(20,5))\n",
    "most_views.plot.bar()\n",
    "plt.title(\"Total pageviews per article\")\n",
    "plt.ylabel(\"Total pageviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have removed all those outlier articles, we have a look at what is the real distribution of pageviews for articles that should represent privacy enhancing techniques. We remark that there is still a high gap in the pageviews, for example, the \"Pretty Good Privacy\" article represents a good percentage of the mass of the distribution.\n",
    "\n",
    "It might look like this tail is quite long, but this is actually because there is a decent amount of articles that were created during the time period of our analysis, and thus don't accumulate as much total views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting total and head 5 articles pageviews\n",
    "To further investigate just how much the top five articles make up of the signal, we create a plot comparing these two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date of reveal\n",
    "reveal = datetime.datetime(2013, 6, 3)\n",
    "fig, ax = plt.subplots(sharey=True, figsize=(10,5))\n",
    "\n",
    "# plot cumulative page views for all the articles in our analysis\n",
    "monthly_en_melt.reset_index().groupby('date').sum().reset_index().plot.scatter(x='date',y='views',ax=ax,label='All articles')\n",
    "\n",
    "# plot cumulative page views for the top five contributors of our signal\n",
    "monthly_en_melt.loc[head].reset_index().groupby('date').sum().reset_index().plot.scatter(x='date',y='views',ax=ax,c='r',label='Top 5 articles')\n",
    "\n",
    "# mark the date of the reveal\n",
    "plt.axvline(reveal,c='r')\n",
    "plt.title(\"Total pageviews for privacy-enhancing technology articles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a lot of variance even before the reveal date, with a peak at the reveal date rather than an effect due to the reveal. We see that the top 5 articles have a very high effect on the trend.\n",
    "\n",
    "Something interesting to note is that in the long term, there seems to be a reversing trend : the pageviews tend to go down until 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the pageviews of articles with the most weight\n",
    "\n",
    "Since the first 5 articles are responsible for a good portion of the trend of the data, we investigate further on these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=5, figsize=(30,5), sharey=True)\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "    monthly_en_melt.loc[head[i]].plot.scatter(x='date',y='views',ax=ax)\n",
    "    ax.axvline(reveal,c='r')\n",
    "    ax.set_title(head[i])\n",
    "plt.suptitle('Article pageviews for top 5 articles (with respect to total views)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the DuckDuckGo article, we are able to see quite an important direct impact on the pageviews, starting from the reveal, however, long term it doesn't seem to affect the trend.\n",
    "\n",
    "For the PGP article, there might be a little bit of an impact, but since there was already a trend of increase it is hard to say.\n",
    "\n",
    "For the SOCKS, PeerGuardian and Freegate articles we can't identify any lasting trends in views from these visualizations which could be attributed to the reveal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmented Regression Analysis\n",
    "Alright, it is time for us to get into the meat and bones of this analysis. We perform a segmented regression analysis on our subset of articles, just like in the original paper we were assigned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "df = monthly_en_melt.groupby('date').sum()\n",
    "\n",
    "# times before and after the reveal\n",
    "before = df.loc[:reveal].reset_index()\n",
    "after  = df.loc[reveal:].reset_index()\n",
    "\n",
    "# set dates to num so regplot is able to produce a result\n",
    "before.date = mdates.date2num(before.date)\n",
    "after.date  = mdates.date2num(after.date)\n",
    "\n",
    "# regplot plots both scatter, and regression fit\n",
    "sns.regplot(x='date', y='views', ax=ax, data=before, label='before')\n",
    "sns.regplot(x='date', y='views', ax=ax, data=after, label='after')\n",
    "\n",
    "# mark reveal\n",
    "plt.axvline(reveal, c='r')\n",
    "plt.legend()\n",
    "plt.title(\"Total pageviews for privacy-enhancing technology articles\")\n",
    "\n",
    "# turn numbers back to dates on the axis\n",
    "loc = mdates.AutoDateLocator()\n",
    "ax.xaxis.set_major_locator(loc)\n",
    "ax.xaxis.set_major_formatter(mdates.AutoDateFormatter(loc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remark a sudden impact in the following few months, but the long term trend seems to be only slightly impacted by the reveal. It even looks like the usual trend of growth becomes more stable starting from the reveal. It is interesting to compare this to the global wikipedia trend to see how it compares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statsmodel Regression Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform regressional analysis and interpret the coefficients of our model here. We make use of the following model in our analysis (like in the original paper):\n",
    "\n",
    "$$\n",
    "Y_t = \\beta_0 + \\beta_1 \\text{time} + \\beta_2 \\text{intervention} + \\beta_3 \\text{postslope} + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start out by preparing a dataframe to have the features of the above-mentioned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe containing features required for regression analysis\n",
    "views = pd.concat([before, after]).views\n",
    "monthly_views = pd.DataFrame([{'views': views.iloc[i], 'month': i} for i in range(len(views))])\n",
    "\n",
    "# event occurred during the middle of this 4 year time period\n",
    "event = 24\n",
    "\n",
    "# provide feature indicating whether event had occurred yet\n",
    "monthly_views.loc[:, 'intervention'] = 1\n",
    "monthly_views.loc[:event, 'intervention'] = 0\n",
    "\n",
    "# provide feature indicating time-delta from event\n",
    "monthly_views.loc[:, 'postslope'] = np.abs(monthly_views.month.values - (event))\n",
    "monthly_views.loc[:event, 'postslope'] = 0\n",
    "monthly_views.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform regression analysis\n",
    "reg = smf.ols('views ~ month + C(intervention) + postslope', data=monthly_views)\n",
    "res = reg.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with the global wikipedia pageviews\n",
    "We will now compare our results with global trends on wikipedia to determine whether there is a difference in trends compared to the global trends seen on wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmented Regression analysis\n",
    "We perform the same analysis as above, using the global page view data. To do so we start out by extracting the global page view data for the English language project. To do so we use the wikimedia API providing these statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pagecounts(language: str, start: str, end: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve the monthly page view aggregates for the given wikipedia language project in the \n",
    "    specified time span.\n",
    "    \n",
    "    :param language: the string representing the language project the article titlse come from\n",
    "    :param start: the string representing the starting date of the requested time span\n",
    "    :param end: the string representing the ending date of the requested time span\n",
    "    \n",
    "    :returns: the resulting page views for the given time span\n",
    "    \"\"\"\n",
    "    r = requests.get(f'https://wikimedia.org/api/rest_v1/metrics/legacy/pagecounts/aggregate/{language}.wikipedia/all-sites/monthly/{start}/{end}')\n",
    "    data = r.json()['items']\n",
    "    \n",
    "    # process each month separately\n",
    "    pagecounts = []\n",
    "    for month in data:\n",
    "        \n",
    "        # add datetime object for ease-of-use\n",
    "        ts = datetime.datetime.strptime(month['timestamp'], '%Y%m%d%H')\n",
    "        pagecounts.append({'Timestamp': ts, 'Page views': month['count']})\n",
    "        \n",
    "    pagecounts = pd.DataFrame(pagecounts).sort_values(by='Timestamp')\n",
    "    return pagecounts\n",
    "\n",
    "monthly_en_all = get_pagecounts('en', '2011063000', '2015063000')\n",
    "monthly_en_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall create the same plot as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event = datetime.datetime(2013, 6, 3)\n",
    "\n",
    "# use mdates for plotting using seaborn\n",
    "monthly_en_all.Timestamp = mdates.date2num(monthly_en_all.Timestamp)\n",
    "\n",
    "# separate groups for before and after regression lines\n",
    "monthly_en_all['Intervention'] = 'Before'\n",
    "monthly_en_all.loc[monthly_en_all.Timestamp > mdates.date2num(event), 'Intervention'] = 'After'\n",
    "\n",
    "# plot regression lines for both time periods\n",
    "g = sns.lmplot(x='Timestamp', y='Page views', hue='Intervention', data=monthly_en_all)\n",
    "\n",
    "# fix labels to use dates instead of mdates\n",
    "labels = pd.Series(g.ax.get_xticks()).map(lambda x: mdates.num2date(x).strftime('%Y-%m-%d')).fillna('')\n",
    "_ = g.set_xticklabels(labels, '', rotation=90)\n",
    "_ = g.ax.set_xlabel('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that there seems to be a very wide confidence interval for the regression line after the event. In general: it would seem as though the page views on Wikipedia are trending upward overall. To make a more quantitative analysis we will need to analyze the underlying model of our plot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StatsModel regression fit\n",
    "We use `statsmodels` to further analyze our fitted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event occurred during the middle of this 4 year time period\n",
    "event = 24\n",
    "views = monthly_en_all['Page views']\n",
    "monthly_views_all = pd.DataFrame([{'views': views.iloc[i], 'month': i} for i in range(len(views))])\n",
    "\n",
    "# provide feature indicating whether event had occurred yet\n",
    "monthly_views_all.loc[:, 'intervention'] = 1\n",
    "monthly_views_all.loc[:event, 'intervention'] = 0\n",
    "\n",
    "# provide feature indicating time-delta from event\n",
    "monthly_views_all.loc[:, 'postslope'] = np.abs(monthly_views_all.month.values - (event))\n",
    "monthly_views_all.loc[:event, 'postslope'] = 0\n",
    "monthly_views_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# let statsmodels do its magic\n",
    "reg = smf.ols('views ~ month + C(intervention) + postslope', data=monthly_views_all)\n",
    "res = reg.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results we must immediately observe that our model doesn't fit the data very well. The $R^2$ is outside the range of values we would have liked to see. If we compare our results with the results achieved in the paper, we see that we attain completely different coefficients from the original author. To validate our data we changed the time span of our analysis temporarily to be the same as in the original study, and we suddenly found a much better model. This raises some concerns about the conclusions of the original study - we would have liked to see an analysis over a longer time span than was performed there.\n",
    "\n",
    "The p-value for the coefficient fitted to the intervention variable is quite high, i.e. the probablity of this variable not having any impact on the page views is almost 50%. Furthermore the postslope also raises questions as the p-value for this coefficient is also quite high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with the same articles in different languages : French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Experiments :\n",
    "\n",
    "We have tried various other ways for dealing with data with a high difference in pageviews, i.e. to not fit our whole hypothesis on the 3 articles that have the most views. The two following experiments didn't lead to significant conclusions but we leave them here for completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Experiment : Standardizing per article\n",
    "\n",
    "In this part, we standardize the views of each articles, thus giving the same weights to all articless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    mean = monthly_en_melt.reset_index().groupby('Article').mean()['views'].loc[x['Article']]\n",
    "    std  = monthly_en_melt.reset_index().groupby('Article').std()['views'].loc[x['Article']]\n",
    "    std = std if std != 0 else 1\n",
    "    return (x['views'] - mean)/std\n",
    "\n",
    "monthly_en_melt = monthly_en_melt.reset_index()\n",
    "monthly_en_melt['standardized'] = monthly_en_melt.apply(standardize,axis=1)\n",
    "monthly_en_melt = monthly_en_melt.set_index('Article')\n",
    "monthly_en_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardized vs total views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "axs[0].axvline(reveal,c='r')\n",
    "axs[1].axvline(reveal,c='r')\n",
    "\n",
    "axs[0].set_title('Standardized')\n",
    "axs[1].set_title('Views')\n",
    "\n",
    "monthly_en_melt.groupby('date').sum().reset_index().plot.scatter(x='date', y='standardized', ax=axs[0], rot=90)\n",
    "monthly_en_melt.groupby('date').sum().reset_index().plot.scatter(x='date', y='views',ax=axs[1],rot=90)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have a high amount of articles, even with our quick preprocessing, we have too many articles that have a low amount of views, which makes them inherently have more variance, so it is unfair to give as much weight to these articles as to the other ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Experiment : Stratification\n",
    "\n",
    "Instead of analyzing a single group of articles that have highly different views, we can select groups of articles that have similar viewcounts, and analyze them together.\n",
    "Given that our distribution of total pageviews per articles can be considered as heavy-tailed, we probably don't want equal width or equal frequency discretization. Instead we opt for clustering to divide our articles into multiple groups, by creating clusters based on the total amount of views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_views = monthly_en_melt.reset_index().groupby('Article').sum().sort_values('views',ascending=False)\n",
    "vals = np.log(sorted_views['views'].values.reshape((-1,1))) #Use log scales values because they are too far away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting number of clusters\n",
    "\n",
    "Since we don't know exactly how many groups we want to define, we will use the silhouette score and the elbow method to help us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sse(features_X, start=2, end=11):\n",
    "    sse = []\n",
    "    for k in range(start, end):\n",
    "        # Assign the labels to the clusters\n",
    "        kmeans = KMeans(n_clusters=k, random_state=10).fit(features_X)\n",
    "        sse.append({\"k\": k, \"sse\": kmeans.inertia_})\n",
    "\n",
    "    sse = pd.DataFrame(sse)\n",
    "    # Plot the data\n",
    "    plt.plot(sse.k, sse.sse)\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Sum of Squared Errors\")\n",
    "    \n",
    "plot_sse(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouettes = []\n",
    "\n",
    "# Try multiple k\n",
    "for k in range(2, 11):\n",
    "    # Cluster the data and assigne the labels\n",
    "    labels = KMeans(n_clusters=k, random_state=10).fit_predict(vals)\n",
    "    # Get the Silhouette score\n",
    "    score = silhouette_score(vals, labels)\n",
    "    silhouettes.append({\"k\": k, \"score\": score})\n",
    "    \n",
    "# Convert to dataframe\n",
    "silhouettes = pd.DataFrame(silhouettes)\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(silhouettes.k, silhouettes.score)\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Silhouette score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there is now particular elbow, we take the value with the best silhouette score, K = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 5\n",
    "\n",
    "clustering = KMeans(n_clusters=NUM_CLUSTERS).fit(vals)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.scatter(x=range(len(sorted_views)),y=sorted_views['views'], c=clustering.labels_)\n",
    "plt.gca().set_yscale('log')\n",
    "plt.xlabel('Article')\n",
    "plt.ylabel('Views')\n",
    "plt.gca().set_xticks(range(len(sorted_views)))\n",
    "plt.gca().set_xticklabels(sorted_views.index, rotation=90)\n",
    "\n",
    "sorted_views['group'] = clustering.labels_\n",
    "monthly_en_melt['group'] = sorted_views['group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reveal = datetime.date(2013,6,5)\n",
    "fig, axs = plt.subplots(nrows=NUM_CLUSTERS, figsize=(10,20))\n",
    "\n",
    "for g,ax in zip(range(len(monthly_en_melt.group.unique())),axs.flatten()):\n",
    "    #Group by date, select the group\n",
    "    selected = monthly_en_melt[monthly_en_melt['group'] == g]\n",
    "    df = selected.groupby('date').sum()\n",
    "\n",
    "    before = df.loc[:reveal].reset_index()\n",
    "    after  = df.loc[reveal:].reset_index()\n",
    "    before.date = mdates.date2num(before.date)\n",
    "    after.date  = mdates.date2num(after.date)\n",
    "    \n",
    "    sns.regplot(x='date',y='views',ax=ax,data=before)\n",
    "    sns.regplot(x='date',y='views',ax=ax,data=after)\n",
    "    \n",
    "    loc = mdates.AutoDateLocator()\n",
    "    ax.xaxis.set_major_locator(loc)\n",
    "    ax.xaxis.set_major_formatter(mdates.AutoDateFormatter(loc))\n",
    "    \n",
    "    sample = selected.reset_index().groupby('Article').sum().sort_values('views', ascending=False).head(3).index\n",
    "    sample_str = ','.join(sample)\n",
    "    ax.set_title(f'Group {g} ({sample_str})')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we are not very comfortable on interpreting these results, since they show many different trends, so we would have to go through each individual article and verify what causes the trend, so we also stopped with this analysis. Another issue we thought of is that the clusters we would find on other languages of wikipedia would be different so it would be hard to compare a general trend given that we find no general trend here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
