{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "import requests\n",
    "import time\n",
    "from typing import List\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.dates as mdates\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.cluster import MeanShift, KMeans\n",
    "from IPython.core.debugger import set_trace\n",
    "from matplotlib import pyplot, dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need data to analyze. From our milestone we can infer that we will need to be able to get pageviews from both specific articles on Wikipedia, as well as the aggregated pageviews for an entire language project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load our generated list of articles\n",
    "For our analysis we have aggregated a set of articles representing \"Internet software privacy\" that we need to analyze. We start out by loading that list of articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load article titles\n",
    "english_names = pd.read_csv('data/articles.csv', header=None, names=['Title'])\n",
    "\n",
    "# transform article names so that we can easily query them in the APIs\n",
    "english_names['Title'] = english_names['Title'].apply(lambda x : x.replace(' ', '_'))\n",
    "english_names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve English pageview data \n",
    "We now utilize https://www.wikishark.com/ to retrieve the actual page view data for each of the articles on the English language Wikipedia project. We have contacted the developer of this website to grant us the permission to call their backends that we found by doing some 1337 hacking. We are allowed to do so for the \"Internet privacy software\"-related articles, if we wait 1-2 seconds in between each request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "def day_year_to_date(year: int, days: int) -> datetime.datetime:\n",
    "    \"\"\"\n",
    "    Takes the given numerical year and days passed of that year \n",
    "    and returns the datetime object representing that date.\n",
    "    \n",
    "    :param year: the christian calendar year representing the date wanted\n",
    "    :param days: the amount of days passed in that year\n",
    "    \n",
    "    :returns: datetime object corresponding to the specified date\n",
    "    \"\"\"\n",
    "    return datetime.datetime(year, 1, 1) + datetime.timedelta(days - 1)\n",
    "\n",
    "def get_wikishark_id(article_name: str, language: str) -> str:\n",
    "    \"\"\"\n",
    "    Gives the wikishark internal ID for the given article and language project.\n",
    "    \n",
    "    :param article_name: the article title for which the internal ID is requested.\n",
    "    :param language: the string representing the language project the article title comes from\n",
    "    \n",
    "    :returns: string corresponding to the retrieved wikishark ID for the title\n",
    "    \"\"\"\n",
    "    response = requests.get(f'https://www.wikishark.com/autocomplete.php?q={article_name}')\n",
    "    r = response.json()\n",
    "    \n",
    "    target = None\n",
    "    for candidate in r:\n",
    "        if '(' + language + ')' in candidate['name'] and article_name.replace('_',' ').lower() in candidate['name'].lower():\n",
    "            return candidate['id']\n",
    "    return target\n",
    "\n",
    "def get_daily_pageviews(titles: List[str], language: str, start: str, end: str, id_provider : Callable[[str,str], str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieves the daily page views for the given articles on the given language edition of Wikipedia.\n",
    "    Filtering is also performed to only return the page views for the given time span.\n",
    "    \n",
    "    :param titles: the list of articles to retrieve page views for\n",
    "    :param language: the string representing the language project the article titlse come from\n",
    "    \n",
    "    :returns: DataFrame containing the daily pageviews for the articles in the time span specified\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for title in tqdm(titles):\n",
    "        \n",
    "        # developer requested we wait 1 second at least in between requests\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # retrieve wikishark ID for article\n",
    "        wikishark_id = id_provider(title, language)\n",
    "        if wikishark_id is None:\n",
    "            print(f'Could not find data for {title}')\n",
    "            continue\n",
    "        \n",
    "        # wait to avoid overloading servers\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # retrieve page views for given title\n",
    "        response = requests.get(f'https://www.wikishark.com/getdata/daily.php?value={wikishark_id}?view=2&scale=0&normalized=0&loglog=0&log=0&zerofix=0')\n",
    "        daily_data = response.json()\n",
    "        \n",
    "        # add data with timestamps\n",
    "        start_date = datetime.datetime.strptime(start, '%d/%m/%Y')\n",
    "        end_date   = datetime.datetime.strptime(end, '%d/%m/%Y')\n",
    "        current_date = datetime.datetime.now()\n",
    "        \n",
    "        # wikishark returns daily page views for every day since 2007-12-31 (independent of given parameters)\n",
    "        # we need to index it according to the time period we are interested in\n",
    "        start_index = (len(daily_data) - 1) - (current_date - start_date).days\n",
    "        end_index = (len(daily_data) - 1) - (current_date - end_date).days\n",
    "        \n",
    "        # add page views for each day\n",
    "        timestamps = {}\n",
    "        for i, d in enumerate(daily_data[start_index:end_index]):\n",
    "            ts = start_date + datetime.timedelta(days=i)\n",
    "            timestamps[ts] = int(d)\n",
    "    \n",
    "        # add data for given article to collection\n",
    "        data.append({**{'Article': title}, **timestamps})\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this next cell we have made it possible to both regenerate the data set yourself, or just load from the pickled object. Given that we have to wait 1 second in between requests to the wikishark API: we would suggest loading the pickled object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def retrieve_or_compute_pageviews(articles, language, start, end, id_provider, pickled_file):\n",
    "    \n",
    "    # load pickled data if available in root of directory\n",
    "    if os.path.isfile(pickled_file):\n",
    "        df = pd.read_pickle(pickled_file)\n",
    "        \n",
    "    # reconstruct data from scratch\n",
    "    else:\n",
    "        #Get daily pageviews\n",
    "        df = get_daily_pageviews(articles, language, start, end, id_provider)\n",
    "        \n",
    "        # use article name as index\n",
    "        df.index = df.Article\n",
    "        df = df.drop(['Article'], axis=1)\n",
    "\n",
    "        # pickle data to avoid having to re-request it\n",
    "        df.to_pickle(pickled_file)\n",
    "        \n",
    "    return df\n",
    "\n",
    "# compute pageviews for the english articles\n",
    "privacy_en = retrieve_or_compute_pageviews(list(english_names.Title),\n",
    "                              'en',\n",
    "                              '01/01/2011',\n",
    "                              '01/01/2016',\n",
    "                              get_wikishark_id,\n",
    "                              'privacy.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate into monthly data\n",
    "Our data from above comes in a daily resolution. Our analysis will be on a monthly basis so we need to aggregate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make columns datetime objects for resampling to work\n",
    "privacy_en.columns = pd.to_datetime(privacy_en.columns)\n",
    "\n",
    "# take monthly cumulative\n",
    "monthly_en = privacy_en.resample('M', axis=1).sum()\n",
    "monthly_en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis \n",
    "We now perform an exploratory data analysis to identify any issues with the data we've generated so far. This is different from the approach taken in the original paper, where issues with outliers were identified and corrected for in another iteration of the analysis.\n",
    "\n",
    "We start by melting our data so that it is easier to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt dataframe to use dates as entry values rather than columns\n",
    "monthly_en_melt = pd.melt(monthly_en, value_name='views', var_name='date', ignore_index=False)\n",
    "\n",
    "# crop dates from 2011-07-01 to 2015-07-01\n",
    "start =  datetime.datetime(2011, 7, 1)\n",
    "end   =  datetime.datetime(2015, 6, 30)\n",
    "\n",
    "# set start and end date, then reset index to article\n",
    "monthly_en_melt = monthly_en_melt.reset_index().set_index('date')[start:end].reset_index().set_index('Article')\n",
    "monthly_en_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a month feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_dict = {pd.Timestamp(date):i for i,date in enumerate(monthly_en_melt.date.unique())}\n",
    "monthly_en_melt['month'] = monthly_en_melt.date.apply(lambda x : date_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a quick look at the summed data real quick:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot by month\n",
    "monthly_en_melt.reset_index().groupby('month').sum().reset_index().plot.scatter(x='month',y='views')\n",
    "plt.xlabel('Month, starting from July 2011')\n",
    "plt.ylabel('Pageviews')\n",
    "plt.title('English wikipedia pageviews for privacy-enhancing technology articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick preprocessing of articles\n",
    "\n",
    "Since we have a lot of articles, we want to first remove articles that are of no interest to us, before we can visualize the total dataset correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all articles that have a very low amount of views no matter what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_views = [article for article in monthly_en_melt.index.unique() if (monthly_en_melt.loc[article].views < 100).all()]\n",
    "print(low_views)\n",
    "monthly_en_melt = monthly_en_melt.drop(low_views)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all articles that may be outliers\n",
    "\n",
    "In the same way the original paper found outlier like the hamas article, we want to remove from our dataset sudden changes in pageviews that are not part of our signal.\n",
    "Since the views an article gets monthly doesn't follow a gaussian distribution, we use IQRs to determine if an article is an outlier rather than just using the 3 standard deviations rule. If the views in a month are higher than the 75th percentile + 3 times the interquartile range, we consider it for manual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_outlier(article, factor=2):\n",
    "    \"\"\"\n",
    "    Computes if the given entry is an outlier article using IQR in our local dataframe.\n",
    "    \"\"\"\n",
    "    q25, q75 = np.percentile(monthly_en_melt.loc[article].views, 25), np.percentile(monthly_en_melt.loc[article].views, 75)\n",
    "    iqr = q75 - q25\n",
    "    cut_off = iqr * factor\n",
    "    lower, upper = q25 - cut_off, q75 + cut_off\n",
    "    return ((monthly_en_melt.loc[article].views < lower) | (monthly_en_melt.loc[article].views > upper)).any()\n",
    "\n",
    "outliers = [article for article in monthly_en_melt.index.unique() if _is_outlier(article)]\n",
    "print(f'Number of outliers identified: {len(outliers)}')\n",
    "print(f'Titles: {outliers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual inspection\n",
    "\n",
    "In order to verify that our potential outliers should be removed, we manually check each of these articles, the news related to it, to find out if there is an obvious reason for the sudden change in views, and if this reason is related to our signal: privacy enhancing technologies.\n",
    "\n",
    "The articles we suspect as outliers are:\n",
    "- **I2P**: Spike happens around the time Silkroad (drug network) moves from tor to I2p\n",
    "- **IMule**: High variance all around, low views, bump in 2012 probably related to the bump in use of i2p in 2012\n",
    "- **Operation onymous**: Spikes around the time of operation, an international law enforcement operation targeting darknet markets and other hidden services operating on the Tor network.\n",
    "- **Tor**: Spike happened when SilkRoad was shut down in october 2013, spike in views was quite large and somewhat unrelated.\n",
    "- **Bitblinder**: Spike happens in  March 2011 when researchers documented an attack that is capable of revealing the IP addresses of BitTorrent users on the Tor network. \n",
    "- **2channel** and **4chan**: Spikes happen in Sep 2015 when the founder of 4chan,  Christopher Poole, formally announced on 21 September 2015 that he had sold the website to the founder of 2channel, \tHiroyuki Nishimura. \n",
    "- **Bitmessage**: Spike happened in Sep 2014 when there were tons of messages being delayed or missing, and also there was word that the network is being attacked by spam. The next update was released then in October 2014 to address these problems.\n",
    "- **Confide**: The pageviews we got from wikishark don't correspond to the article about the Confide app, but another article with the same name (disambiguation is not working very well on wikishark)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier removal\n",
    "We remove the aforementioned articles from our analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_outliers = ['4chan','I2P', 'IMule', 'Operation_Onymous', 'Tor_(anonymity_network)', '2channel', 'Bitmessage', 'Confide']\n",
    "monthly_en_melt= monthly_en_melt.drop(manual_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally some data visualization\n",
    "We produce a set of plots to get more of a feel of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting per article views (total)\n",
    "This will let us see the page view contribution of each article to the overall signal we are analyzing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort articles by total views in time period\n",
    "most_views = monthly_en_melt.reset_index().groupby('Article')['views'].sum().sort_values(ascending=False)\n",
    "\n",
    "# keep the index for the five articles with the most views\n",
    "head = list(most_views.head(5).index) # a surprise tool that will help us later\n",
    "\n",
    "# plot the cumulative views for the articles\n",
    "plt.figure(figsize=(20,5))\n",
    "most_views.plot.bar()\n",
    "\n",
    "# labels\n",
    "plt.title(\"Total pageviews per article\")\n",
    "plt.ylabel(\"Total pageviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have removed all those outlier articles, we have a look at what is the real distribution of pageviews for articles that should represent privacy enhancing techniques. We remark that there is still a high gap in the pageviews, for example, the \"Pretty Good Privacy\" article represents a good percentage of the mass of the distribution.\n",
    "\n",
    "It might look like this tail is quite long, but this is actually because there is a decent amount of articles that were created during the time period of our analysis, and thus don't accumulate as much total views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting total and head 5 articles pageviews\n",
    "To further investigate just how much the top five articles make up of the signal, we create a plot comparing these two:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date of reveal\n",
    "reveal_index = monthly_en_melt[(monthly_en_melt.date.dt.month == 6) & (monthly_en_melt.date.dt.year == 2013)].month.iloc[0]\n",
    "fig, ax = plt.subplots(sharey=True, figsize=(10,5))\n",
    "\n",
    "# plot cumulative page views for all the articles in our analysis\n",
    "monthly_en_melt.reset_index().groupby('month').sum().reset_index().plot.scatter(x='month',y='views',ax=ax,label='All articles')\n",
    "\n",
    "# plot cumulative page views for the top five contributors of our signal\n",
    "monthly_en_melt.loc[head].reset_index().groupby('month').sum().reset_index().plot.scatter(x='month',y='views',ax=ax,c='r',label='Top 5 articles')\n",
    "\n",
    "# plot reveal line between may and june 2011\n",
    "plt.axvline(reveal_index-0.5,c='r')\n",
    "\n",
    "# labels, legend\n",
    "plt.title(\"Total pageviews for privacy-enhancing technology articles\")\n",
    "plt.xlabel('Month, starting from July 2011')\n",
    "plt.ylabel('Pageviews')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a lot of variance even before the reveal date, with a peak at the reveal date rather than an effect due to the reveal. We see that the top 5 articles have a very high effect on the trend.\n",
    "\n",
    "Something interesting to note is that in the long term, there seems to be a reversing trend : the pageviews tend to go down until 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the pageviews of articles with the most weight\n",
    "\n",
    "Since the first 5 articles are responsible for a good portion of the trend of the data, we investigate further on these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=5, figsize=(30,5), sharey=True)\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "    \n",
    "    # plot for each top article\n",
    "    monthly_en_melt.loc[head[i]].plot.scatter(x='month',y='views',ax=ax)\n",
    "    \n",
    "    # plot reveal line between may and june 2011\n",
    "    ax.axvline(reveal_index-0.5,c='r')\n",
    "    \n",
    "    # labels, axis names\n",
    "    ax.set_title(head[i])\n",
    "    ax.set_xlabel('Month, starting from July 2011')\n",
    "    ax.set_ylabel('Pageviews')\n",
    "plt.suptitle('Article pageviews for top 5 articles (with respect to total views)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the DuckDuckGo article, we are able to see quite an important direct impact on the pageviews, starting from the reveal, however, long term it doesn't seem to affect the trend.\n",
    "\n",
    "For the PGP article, there might be a little bit of an impact, but since there was already a trend of increase it is hard to say.\n",
    "\n",
    "For the SOCKS, PeerGuardian and Freegate articles we can't identify any lasting trends in views from these visualizations which could be attributed to the reveal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmented Regression Analysis\n",
    "Alright, it is time for us to get into the meat and bones of this analysis. We perform a segmented regression analysis on our subset of articles, just like in the original paper we were assigned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(16,9))\n",
    "\n",
    "# take total views\n",
    "df = monthly_en_melt.groupby('month').sum()\n",
    "\n",
    "# times before and after the reveal\n",
    "before = df.loc[:reveal_index-1].reset_index()\n",
    "after  = df.loc[reveal_index:].reset_index()\n",
    "\n",
    "# regplot plots both scatter, and regression fit\n",
    "sns.regplot(x='month', y='views', ax=ax, data=before, label='Before')\n",
    "sns.regplot(x='month', y='views', ax=ax, data=after, label='After')\n",
    "\n",
    "# plot reveal line between may and june 2011\n",
    "plt.axvline(reveal_index-0.5, c='r')\n",
    "\n",
    "# labels, legend\n",
    "plt.legend()\n",
    "plt.title(\"Total pageviews for privacy-enhancing technology articles\")\n",
    "plt.xlabel('Month, starting July 2011')\n",
    "plt.ylabel('Page views')\n",
    "\n",
    "# change number of ticks on y axis\n",
    "plt.locator_params(axis='y', nbins=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remark a sudden impact in the following few months, but the long term trend seems to be only slightly impacted by the reveal. It even looks like the usual trend of growth becomes more stable starting from the reveal. It is interesting to compare this to the global wikipedia trend to see how it compares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statsmodel Regression Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform regressional analysis and interpret the coefficients of our model here. We make use of the following model in our analysis (like in the original paper):\n",
    "\n",
    "$$\n",
    "Y_t = \\beta_0 + \\beta_1 \\text{time} + \\beta_2 \\text{intervention} + \\beta_3 \\text{postslope} + \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start out by preparing a dataframe to have the features of the above-mentioned model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe containing features required for regression analysis\n",
    "views = pd.concat([before, after]).views\n",
    "monthly_views = pd.DataFrame([{'views': views.iloc[i], 'month': i} for i in range(len(views))])\n",
    "\n",
    "# event occurred during the middle of this 4 year time period\n",
    "event = reveal_index - 1\n",
    "\n",
    "# provide feature indicating whether event had occurred yet\n",
    "monthly_views.loc[:, 'intervention'] = 1\n",
    "monthly_views.loc[:event, 'intervention'] = 0\n",
    "\n",
    "# provide feature indicating time-delta from event\n",
    "monthly_views.loc[:, 'postslope'] = np.abs(monthly_views.month.values - (event+1))\n",
    "monthly_views.loc[:event, 'postslope'] = 0\n",
    "monthly_views.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform regression analysis\n",
    "reg = smf.ols('views ~ month + C(intervention) + postslope', data=monthly_views)\n",
    "res = reg.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total security page views had an immediate increase of 24780 following June 2013. This is about 12.4% immediate increase. However, the p-value the C(intervention) suggest that this increase may not be statistically significant. \n",
    "Moreover, the slop of the increasing trend in the monthly page views have decreased. In this case too, the p-value suggests that the results are not statistically significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with the global wikipedia pageviews\n",
    "We will now compare our results with global trends on wikipedia to determine whether there is a difference in trends compared to the global trends seen on wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmented Regression analysis\n",
    "We perform the same analysis as above, using the global page view data. To do so we start out by extracting the global page view data for the English language project. To do so we use the wikimedia API providing these statistics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pagecounts(language: str, start: str, end: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retrieve the monthly page view aggregates for the given wikipedia language project in the \n",
    "    specified time span.\n",
    "    \n",
    "    :param language: the string representing the language project the article titlse come from\n",
    "    :param start: the string representing the starting date of the requested time span\n",
    "    :param end: the string representing the ending date of the requested time span\n",
    "    \n",
    "    :returns: the resulting page views for the given time span\n",
    "    \"\"\"\n",
    "    r = requests.get(f'https://wikimedia.org/api/rest_v1/metrics/legacy/pagecounts/aggregate/{language}.wikipedia/all-sites/monthly/{start}/{end}')\n",
    "    data = r.json()['items']\n",
    "    \n",
    "    # process each month separately\n",
    "    pagecounts = []\n",
    "    for month in data:\n",
    "        \n",
    "        # add datetime object for ease-of-use\n",
    "        ts = datetime.datetime.strptime(month['timestamp'], '%Y%m%d%H')\n",
    "        pagecounts.append({'Timestamp': ts, 'Page views': month['count']})\n",
    "        \n",
    "    pagecounts = pd.DataFrame(pagecounts).sort_values(by='Timestamp')\n",
    "    return pagecounts\n",
    "\n",
    "monthly_en_all = get_pagecounts('en', '2011063000', '2015063000')\n",
    "monthly_en_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_dict = {pd.Timestamp(date):i for i,date in enumerate(monthly_en_all.Timestamp.unique())}\n",
    "monthly_en_all['month'] = monthly_en_all.Timestamp.apply(lambda x : date_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we shall create the same plot as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(16,9))\n",
    "\n",
    "df = monthly_en_all.copy()\n",
    "\n",
    "june_first = datetime.datetime(2013,6,1)\n",
    "\n",
    "# times before and after the reveal\n",
    "before_all = df[df.month < reveal_index]\n",
    "after_all  = df[df.month >= reveal_index]\n",
    "\n",
    "# plot regression lines for both time periods\n",
    "g = sns.regplot(x='month', y='Page views', data=before_all, label='Before')\n",
    "g = sns.regplot(x='month', y='Page views', data=after_all, label='After')\n",
    "\n",
    "# turn numbers back to dates on the axis\n",
    "ax.get_yaxis().get_major_formatter().set_scientific(True)\n",
    "\n",
    "# plot reveal line between may and june 2011\n",
    "plt.axvline(reveal_index-0.5, c='r')\n",
    "\n",
    "#label axis, legend\n",
    "plt.legend(loc='upper left')\n",
    "plt.title(\"Total pageviews over all Wikipedia\")\n",
    "plt.xlabel('Month, starting July 2011')\n",
    "plt.ylabel('Page views')\n",
    "plt.locator_params(axis='y', nbins=6);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that there seems to be a very wide confidence interval for the regression line after the event. In general: it would seem as though the page views on Wikipedia are trending upward overall. To make a more quantitative analysis we will need to analyze the underlying model of our plot:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StatsModel regression fit\n",
    "We use `statsmodels` to further analyze our fitted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# event occurred during the middle of this 4 year time period\n",
    "event = reveal_index - 1\n",
    "views = monthly_en_all['Page views']\n",
    "monthly_views_all = pd.DataFrame([{'views': views.iloc[i], 'month': i} for i in range(len(views))])\n",
    "\n",
    "# provide feature indicating whether event had occurred yet\n",
    "monthly_views_all.loc[:, 'intervention'] = 1\n",
    "monthly_views_all.loc[:event, 'intervention'] = 0\n",
    "\n",
    "# provide feature indicating time-delta from event\n",
    "monthly_views_all.loc[:, 'postslope'] = np.abs(monthly_views_all.month.values - (event+1))\n",
    "monthly_views_all.loc[:event, 'postslope'] = 0\n",
    "monthly_views_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let statsmodels do its magic\n",
    "reg = smf.ols('views ~ month + C(intervention) + postslope', data=monthly_views_all)\n",
    "res = reg.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results we must immediately observe that our model doesn't fit the data very well. The $R^2$ is outside the range of values we would have liked to see. If we compare our results with the results achieved in the paper, we see that we attain completely different coefficients from the original author. To validate our data we changed the time span of our analysis temporarily to be the same as in the original study, and we suddenly found a much better model. This raises some concerns about the conclusions of the original study - we would have liked to see an analysis over a longer time span than was performed there.\n",
    "\n",
    "The p-value for the coefficient fitted to the intervention variable is quite high, i.e. the probablity of this variable not having any impact on the page views is almost 50%. Furthermore the postslope also raises questions as the p-value for this coefficient is also quite high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with the same articles in different languages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve wikidata entries for each given article\n",
    "\n",
    "We proceed by getting the wikidata IDs for each article that remains in our list of articles. We need these so that we can retrieve the equivalent articles in other language projects of Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qid_from_title(title: str, language: str) -> str:\n",
    "    \"\"\"\n",
    "    Gets the Wikidata ID for the given article on the given language project.\n",
    "    \n",
    "    :param title: the article title to retrieve qid for\n",
    "    :param language: the string representing the language project the article title comes from\n",
    "    \n",
    "    :returns: the Wikidata ID for the article\n",
    "    \"\"\"\n",
    "    response = requests.get(f'https://{language}.wikipedia.org/w/api.php?'\n",
    "                            f'action=query&prop=pageprops&titles={title}&redirects&format=json')\n",
    "    try:\n",
    "        r = [item for item in response.json()['query']['pages'].values()][0]\n",
    "        qid = r['pageprops']['wikibase_item']\n",
    "    except KeyError:\n",
    "        print(f'Article {title} has no Wikidata ID')\n",
    "        return None\n",
    "    return qid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only retrieve qids for articles we have kept throughout our first analysis, so as to not lose time again during preprocessing, as it is very unlikely that an article never got traction in english (never more than 100 views a month), but did in german wikipedia (given how much more pageviews the english wikipedia gets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_qids = pd.DataFrame([get_qid_from_title(title, 'en') for title in tqdm(monthly_en_melt.index.unique())])\n",
    "len(remaining_qids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All articles do have a wikidata entry associated to them, we still have 44 articles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve titles from other languages from our list of qids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_from_qid(qid,language):\n",
    "    response = requests.get(f'https://www.wikidata.org/w/api.php?action=wbgetentities&format=xml&props=sitelinks&ids={qid}&sitefilter={language}wiki&format=json')\n",
    "    try:\n",
    "        title = response.json()['entities'][qid]['sitelinks'][language+'wiki']['title']\n",
    "        return title\n",
    "    except:\n",
    "        print(f'Article with ID {qid} does not have Wikipedia page in {language}.')\n",
    "        return None\n",
    "#example code: get_title_from_qid(qid='Q19675',language='fr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the pageviews for titles in other languages\n",
    "\n",
    "Because the API we had previously used to get the identifier given to articles by wikishark doesn't work consistently when searching for titles in other languages, we had to find a more ad-hoc way : we scrape the webpage that would normally be accessed by humans in order to find the webpage of the article (ex : https://www.wikishark.com/title/fr/DuckDuckGo for DuckDuckGo), and from this webpage, scrap the ID which is found in the source of the webpage. It is not the most pretty way, but it is the only way we found to gather the most amount of articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_wikishark_addr_from_title(title, language):\n",
    "    \"\"\"Returns the wikishark url for the corresponding title and language\"\"\"\n",
    "    DEFAULT_URL = \"https://www.wikishark.com\"\n",
    "    URL = \"https://www.wikishark.com/search/{} ({})\".format(title, language)\n",
    "    req_text = requests.get(URL).text\n",
    "    soup = BeautifulSoup(req_text, 'html.parser')\n",
    "    \n",
    "    # get list of all articles found\n",
    "    lis = soup.find_all('li', attrs={'style': 'visibility: visible;'})\n",
    "    for li in lis:\n",
    "        for a in li.p.findChildren('a', recursive=False):\n",
    "            \n",
    "            # check that the article contains the title, and language\n",
    "            if language in a.text and title.lower() in a['title'].lower():\n",
    "                return DEFAULT_URL + a['href']\n",
    "        \n",
    "def get_wikishark_id_from_addr(addr, title):\n",
    "    \"\"\"Returns the wikishark identifier for the corresponding article title and url address\"\"\"\n",
    "    req_text = requests.get(addr).text\n",
    "    soup = BeautifulSoup(req_text, 'html.parser')\n",
    "    \n",
    "    # find all scripts\n",
    "    scripts = soup.find_all('script', attrs={'type':\"text/javascript\"})\n",
    "    id_script = None\n",
    "    for script in scripts:\n",
    "        \n",
    "        # only keep scripts that contain the title\n",
    "        if script.string is not None and title.lower() in script.string.lower():\n",
    "            id_script = script.string\n",
    "            \n",
    "    if id_script is not None:\n",
    "        \n",
    "        # match all dictionaries inside script\n",
    "        dic_matcher = r'(\\{[^{}]+\\})'\n",
    "        for match in re.findall(dic_matcher, id_script):\n",
    "            num_matcher = r'\\d+'\n",
    "            \n",
    "            # match all identifiers (numbers)\n",
    "            for num in re.findall(num_matcher, match):\n",
    "                try:\n",
    "                    return int(num)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "def wikishark_id_from_title(title, language):\n",
    "    \"\"\"Returns the wikishark identifier for the corresponding title and language\"\"\"\n",
    "    #first get article url\n",
    "    url = get_wikishark_addr_from_title(title, language)\n",
    "    if url is None:\n",
    "        return None\n",
    "    \n",
    "    #sleep so as to not overflow their datacenter with requests\n",
    "    time.sleep(1)\n",
    "    \n",
    "    #then get id from url\n",
    "    return get_wikishark_id_from_addr(url, title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study : German wikipedia\n",
    "\n",
    "We chose to study german wikipedia instead of french wikipedia because german is the second wikipedia language project with respect to pageviews (the first being obviously english)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "german_titles = [get_title_from_qid(qid,'de') for qid in np.ravel(remaining_qids.values)]\n",
    "german_titles = [title for title in german_titles if title is not None]\n",
    "len(german_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot of articles don't exist in german, thus we have only at most 22 articles for which we can get the pageviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_de = retrieve_or_compute_pageviews(german_titles,\n",
    "                              'German',\n",
    "                              '01/01/2011',\n",
    "                              '01/01/2016',\n",
    "                              wikishark_id_from_title,\n",
    "                              'privacy_de.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_de.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make columns datetime objects for resampling to work\n",
    "privacy_de.columns = pd.to_datetime(privacy_de.columns)\n",
    "\n",
    "# take monthly cumulative\n",
    "monthly_de = privacy_de.resample('M', axis=1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melting data, and only keep data in the same timeframe as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt dataframe to use dates as entry values rather than columns\n",
    "monthly_de_melt = pd.melt(monthly_de, value_name='views', var_name='date', ignore_index=False)\n",
    "\n",
    "#Crop dates from 2011-07-01 to 2015-07-01\n",
    "start =  datetime.datetime(2011, 7, 1)\n",
    "end   =  datetime.datetime(2015, 6, 30)\n",
    "monthly_de_melt = monthly_de_melt.reset_index().set_index('date')[start:end].reset_index().set_index('Article')\n",
    "monthly_de_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a month feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_dict = {pd.Timestamp(date):i for i,date in enumerate(monthly_de_melt.date.unique())}\n",
    "monthly_de_melt['month'] = monthly_de_melt.date.apply(lambda x : date_dict[x])\n",
    "monthly_de_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting total pageviews views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_views = monthly_de_melt.reset_index().groupby('month').sum().reset_index()\n",
    "\n",
    "# plot each month \n",
    "de_views.plot.scatter(x='month',y='views')\n",
    "\n",
    "# mark outlier month in red\n",
    "de_views.iloc[[31]].plot.scatter(x='month',y='views',c='r',ax=plt.gca())\n",
    "\n",
    "# plot reveal line between may and june 2011\n",
    "plt.gca().axvline(reveal_index-0.5,c='r')\n",
    "plt.title('Total german pageviews')\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks reasonably promising, although we might see what can look like at outlier in February 2014, which we marked in red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting per article views (total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort articles by total views in time period\n",
    "most_views = monthly_de_melt.reset_index().groupby('Article')['views'].sum().sort_values(ascending=False)\n",
    "\n",
    "# keep the index for the five articles with the most views\n",
    "head = list(most_views.head(5).index) # a surprise tool that will help us later\n",
    "\n",
    "# plot the cumulative views for the articles\n",
    "plt.figure(figsize=(20,5))\n",
    "most_views.plot.bar()\n",
    "plt.title(\"Total pageviews per article\")\n",
    "plt.ylabel(\"Total pageviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the pageviews for the top 5 articles\n",
    "\n",
    "Since the top 5 articles gave us a lot of information in our first study, lets plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=5, figsize=(30,5), sharey=True)\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "    \n",
    "    # plot each of the top 5 articles\n",
    "    monthly_de_melt.loc[head[i]].plot.scatter(x='month',y='views',ax=ax)\n",
    "    \n",
    "    # plot reveal line between may and june 2011\n",
    "    ax.axvline(reveal_index,c='r')\n",
    "    ax.set_title(head[i])\n",
    "plt.suptitle('Article pageviews for top 5 articles (with respect to total views)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the Threema article is the reason why we have such a high change in February 2014, why could that be? This date coincides with the endorsement of Threema by the Stiftung Warentest. This event is listed here : https://en.wikipedia.org/wiki/Threema#Reception. Since it is not an effect of the PRISM reveal, we remove this article from our list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_threema = monthly_de_melt.drop('Threema')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the total views with Threema removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot total views for all articles (without threema)\n",
    "no_threema.reset_index().groupby('month').sum().reset_index().plot.scatter(x='month',y='views', label='All')\n",
    "\n",
    "#Plot reveal line between may and june 2011\n",
    "plt.axvline(reveal_index-0.5)\n",
    "ax = plt.gca()\n",
    "\n",
    "#Plot total views for top 2 articles\n",
    "no_threema.loc[['DuckDuckGo','Pretty Good Privacy']].reset_index().groupby('month').sum().reset_index().plot.scatter(ax=ax,x='month',y='views',c='r', label='DuckDuckGo & PGP')\n",
    "plt.legend()\n",
    "plt.title('German pageviews');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks interesting, as the sudden impact from the reveal can be observed directly. However, we see another issue, most of the trend can be explained simply by the pageviews of the two articles with the highest amount of pageviews : DuckDuckGo and Pretty Good Privacy. We will try to deal with this issue later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_threema.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(16,9))\n",
    "df = no_threema.groupby('month').sum().reset_index()\n",
    "\n",
    "# times before and after the reveal\n",
    "before = df[df.month <  reveal_index].reset_index()\n",
    "after  = df[df.month >= reveal_index].reset_index()\n",
    "\n",
    "\n",
    "# regplot plots both scatter, and regression fit\n",
    "sns.regplot(x='month', y='views', ax=ax, data=before, label='Before')\n",
    "sns.regplot(x='month', y='views', ax=ax, data=after, label='After')\n",
    "\n",
    "# plot reveal line between may and june 2011\n",
    "plt.axvline(reveal_index-0.5, c='r')\n",
    "plt.legend()\n",
    "plt.title(\"Total german pageviews for privacy-enhancing technology articles\")\n",
    "\n",
    "# turn numbers back to dates on the axis\n",
    "plt.margins(x=0.5)\n",
    "plt.xlabel('Month, starting July 2011')\n",
    "plt.ylabel('Page views')\n",
    "\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"sci\", scilimits=(0,0))\n",
    "plt.locator_params(axis='y', nbins=7);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StatsModel regression fit\n",
    "We use `statsmodels` to further analyze our fitted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe containing features required for regression analysis\n",
    "de_views = pd.concat([before, after]).views\n",
    "monthly_de_views = pd.DataFrame([{'views': de_views.iloc[i], 'month': i} for i in range(len(de_views))])\n",
    "\n",
    "# event occurred during the middle of this 4 year time period\n",
    "event = reveal_index-1\n",
    "\n",
    "# provide feature indicating whether event had occurred yet\n",
    "monthly_de_views.loc[:, 'intervention'] = 1\n",
    "monthly_de_views.loc[:event, 'intervention'] = 0\n",
    "\n",
    "# provide feature indicating time-delta from event\n",
    "monthly_de_views.loc[:, 'postslope'] = np.abs(monthly_de_views.month.values - (event+1))\n",
    "monthly_de_views.loc[:event, 'postslope'] = 0\n",
    "monthly_de_views.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform regression analysis\n",
    "reg = smf.ols('views ~ month + C(intervention) + postslope', data=monthly_de_views)\n",
    "res = reg.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This $R^2$ score is not very high, which means that the linear fit cannot explain properly the variance we see here. Moreover, the p-value for the $\\beta_1$ coefficient is 0.562 which is very high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also removing DuckDuckGo and Pretty Good Privacy ?\n",
    "\n",
    "An issue we have seen before is that the \"DuckDuckGo\" and \"Pretty Good Privacy\" articles are able to explain most of the trend in the german pageviews. Here, we try removing them, to see if there is a trend among the articles that have a lower amount of views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_big_article = monthly_de_melt.drop(['Threema', 'DuckDuckGo', 'Pretty Good Privacy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_big_article.reset_index().groupby('month').sum().reset_index().plot.scatter(x='month',y='views')\n",
    "plt.title('German pageviews without Threema, DuckDuckGo or Pretty Good Privacy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't see any visual trend in this data, so we do not engage further in this experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Experiments :\n",
    "\n",
    "We have tried various other ways for dealing with data with a high difference in pageviews, i.e. to not fit our whole hypothesis on the 3 articles that have the most views. The two following experiments didn't lead to significant conclusions but we leave them here for completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Experiment : Standardizing per article\n",
    "\n",
    "In this part, we standardize the views of each articles, thus giving the same weights to all articless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    mean = monthly_en_melt.reset_index().groupby('Article').mean()['views'].loc[x['Article']]\n",
    "    std  = monthly_en_melt.reset_index().groupby('Article').std()['views'].loc[x['Article']]\n",
    "    std = std if std != 0 else 1\n",
    "    return (x['views'] - mean)/std\n",
    "\n",
    "# apply standardization over all articles\n",
    "monthly_en_melt = monthly_en_melt.reset_index()\n",
    "monthly_en_melt['standardized'] = monthly_en_melt.apply(standardize,axis=1)\n",
    "\n",
    "# reset index back to articles\n",
    "monthly_en_melt = monthly_en_melt.set_index('Article')\n",
    "monthly_en_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardized vs total views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "# plot reveal line between may and june 2011\n",
    "axs[0].axvline(reveal_index-0.5,c='r')\n",
    "axs[1].axvline(reveal_index-0.5,c='r')\n",
    "\n",
    "axs[0].set_title('Standardized')\n",
    "axs[1].set_title('Views')\n",
    "\n",
    "# plot standardized and normal views\n",
    "monthly_en_melt.groupby('month').sum().reset_index().plot.scatter(x='month', y='standardized', ax=axs[0], rot=90)\n",
    "monthly_en_melt.groupby('month').sum().reset_index().plot.scatter(x='month', y='views',ax=axs[1],rot=90)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have a high amount of articles, even with our quick preprocessing, we have too many articles that have a low amount of views, which makes them inherently have more variance, so it is unfair to give as much weight to these articles as to the other ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Experiment : Stratification\n",
    "\n",
    "Instead of analyzing a single group of articles that have highly different views, we can select groups of articles that have similar viewcounts, and analyze them together.\n",
    "Given that our distribution of total pageviews per articles can be considered as heavy-tailed, we probably don't want equal width or equal frequency discretization. Instead we opt for clustering to divide our articles into multiple groups, by creating clusters based on the total amount of views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_views = monthly_en_melt.reset_index().groupby('Article').sum().sort_values('views',ascending=False)\n",
    "vals = np.log(sorted_views['views'].values.reshape((-1,1))) #Use log scales values because they are too far away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting number of clusters\n",
    "\n",
    "Since we don't know exactly how many groups we want to define, we will use the silhouette score and the elbow method to help us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sse(features_X, start=2, end=11):\n",
    "    sse = []\n",
    "    for k in range(start, end):\n",
    "        # Assign the labels to the clusters\n",
    "        kmeans = KMeans(n_clusters=k, random_state=10).fit(features_X)\n",
    "        sse.append({\"k\": k, \"sse\": kmeans.inertia_})\n",
    "\n",
    "    sse = pd.DataFrame(sse)\n",
    "    # Plot the data\n",
    "    plt.plot(sse.k, sse.sse)\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Sum of Squared Errors\")\n",
    "    \n",
    "plot_sse(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouettes = []\n",
    "\n",
    "# Try multiple k\n",
    "for k in range(2, 11):\n",
    "    # Cluster the data and assigne the labels\n",
    "    labels = KMeans(n_clusters=k, random_state=10).fit_predict(vals)\n",
    "    # Get the Silhouette score\n",
    "    score = silhouette_score(vals, labels)\n",
    "    silhouettes.append({\"k\": k, \"score\": score})\n",
    "    \n",
    "# Convert to dataframe\n",
    "silhouettes = pd.DataFrame(silhouettes)\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(silhouettes.k, silhouettes.score)\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Silhouette score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there is now particular elbow, we take the value with the best silhouette score, K = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 5\n",
    "\n",
    "# fit clustering to data\n",
    "clustering = KMeans(n_clusters=NUM_CLUSTERS).fit(vals)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "\n",
    "#plot views with the right label\n",
    "plt.scatter(x=range(len(sorted_views)),y=sorted_views['views'], c=clustering.labels_)\n",
    "\n",
    "#set scale and labels\n",
    "plt.gca().set_yscale('log')\n",
    "plt.xlabel('Article')\n",
    "plt.ylabel('Views')\n",
    "\n",
    "#set and rotate xticks\n",
    "plt.gca().set_xticks(range(len(sorted_views)))\n",
    "plt.gca().set_xticklabels(sorted_views.index, rotation=90)\n",
    "\n",
    "#set group of all articles using clustering labels\n",
    "sorted_views['group'] = clustering.labels_\n",
    "monthly_en_melt['group'] = sorted_views['group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reveal = datetime.date(2013,6,5)\n",
    "fig, axs = plt.subplots(nrows=NUM_CLUSTERS, figsize=(10,20))\n",
    "\n",
    "for g,ax in zip(range(len(monthly_en_melt.group.unique())),axs.flatten()):\n",
    "    \n",
    "    # group by date, select the group\n",
    "    selected = monthly_en_melt[monthly_en_melt['group'] == g]\n",
    "    df = selected.groupby('month').sum().reset_index()\n",
    "    \n",
    "    # separate in before and after reveal date\n",
    "    before = df[df.month < reveal_index].reset_index()\n",
    "    after  = df[df.month >= reveal_index].reset_index()\n",
    "    \n",
    "    # plot both regression plots \n",
    "    sns.regplot(x='month',y='views',ax=ax,data=before)\n",
    "    sns.regplot(x='month',y='views',ax=ax,data=after)\n",
    "    \n",
    "    # edit title to top article names\n",
    "    sample = selected.reset_index().groupby('Article').sum().sort_values('views', ascending=False).head(3).index\n",
    "    sample_str = ','.join(sample)\n",
    "    ax.set_title(f'Group {g} ({sample_str})')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we are not very comfortable on interpreting these results, since they show many different trends, so we would have to go through each individual article and verify what causes the trend, so we also stopped with this analysis. Another issue we thought of is that the clusters we would find on other languages of wikipedia would be different so it would be hard to compare a general trend given that we find no general trend here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# French wikipedia pageview analysis\n",
    "\n",
    "We tried analyzing the french wikipedia, but could not find any particular trend change from the reveal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "french_titles  = [get_title_from_qid(qid,'fr') for qid in np.ravel(remaining_qids.values)]\n",
    "french_titles = [title for title in french_titles if title is not None]\n",
    "french_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve pageviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_fr = retrieve_or_compute_pageviews(french_titles,\n",
    "                              'French',\n",
    "                              '01/01/2011',\n",
    "                              '01/01/2016',\n",
    "                              wikishark_id_from_title,\n",
    "                              'privacy_fr.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot total monthly pageviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make columns datetime objects for resampling to work\n",
    "privacy_fr.columns = pd.to_datetime(privacy_fr.columns)\n",
    "\n",
    "# take monthly cumulative\n",
    "monthly_fr = privacy_fr.resample('M', axis=1).sum()\n",
    "monthly_fr.sum().to_frame().reset_index().plot.scatter(x='index',y=0)\n",
    "plt.xlabel('Month, starting July 2011')\n",
    "plt.ylabel('Pageviews')\n",
    "plt.title('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melt dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_fr_melt = pd.melt(monthly_fr, value_name='views', var_name='date', ignore_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_dict = {pd.Timestamp(date):i for i,date in enumerate(monthly_fr_melt.date.unique())}\n",
    "monthly_fr_melt['month'] = monthly_fr_melt.date.apply(lambda x : date_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find Distribution of pageviews by article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort articles by total views in time period\n",
    "most_views = monthly_fr_melt.reset_index().groupby('Article')['views'].sum().sort_values(ascending=False)\n",
    "\n",
    "# keep the index for the five articles with the most views\n",
    "head = list(most_views.head(5).index) # a surprise tool that will help us later\n",
    "\n",
    "# plot the cumulative views for the articles\n",
    "plt.figure(figsize=(20,5))\n",
    "most_views.plot.bar()\n",
    "plt.title(\"Total pageviews per article\")\n",
    "plt.ylabel(\"Total pageviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot pageviews for the top 5 articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=5, figsize=(30,5), sharey=True)\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "    \n",
    "    # for each of the top 5 article, plot it\n",
    "    monthly_fr_melt.loc[head[i]].plot.scatter(x='month',y='views',ax=ax)\n",
    "    \n",
    "    # plot reveal line between may and june 2011\n",
    "    ax.axvline(reveal_index-0.5,c='r')\n",
    "    ax.set_title(head[i])\n",
    "plt.suptitle('Article pageviews for top 5 articles (with respect to total views)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing the two biggest articles with respect to pageviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_big_fr = monthly_fr_melt.drop(['DuckDuckGo', 'Pretty Good Privacy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_big_fr.reset_index().groupby('month').sum().reset_index().plot.scatter(x='month',y='views')\n",
    "plt.title('French pageviews without \"DuckDuckGo\" and \"Pretty Good Privacy\" articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
