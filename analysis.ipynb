{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.core.debugger import set_trace\n",
    "import datetime\n",
    "import requests\n",
    "import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_names = pd.read_csv('data/articles.csv', header=None)\n",
    "english_names.columns = ['Title']\n",
    "english_names['Title'] = english_names['Title'].apply(lambda x : x.replace(' ', '_'))\n",
    "english_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qid_from_title(title, language):\n",
    "    response = requests.get(f'https://{language}.wikipedia.org/w/api.php?'\n",
    "                            f'action=query&prop=pageprops&titles={title}&redirects&format=json')\n",
    "    try:\n",
    "        r = [item for item in response.json()['query']['pages'].values()][0]\n",
    "        qid = r['pageprops']['wikibase_item']\n",
    "    except KeyError:\n",
    "        print(f'Article {title} has no Wikidata ID')\n",
    "        return None\n",
    "    return qid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve wikidata entries for each given article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qids = pd.DataFrame([get_qid_from_title(title, 'en') for title in tqdm(english_names.Title.values)])\n",
    "qids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English pageview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_year_to_date(year, days):\n",
    "    return datetime.datetime(year, 1, 1) + datetime.timedelta(days - 1)\n",
    "\n",
    "def get_wikishark_id(article_name, lang):\n",
    "    response = requests.get(f'https://www.wikishark.com/autocomplete.php?q={article_name}')\n",
    "    r = response.json()\n",
    "    \n",
    "    target = None\n",
    "    for candidate in r:\n",
    "        if '(' + lang + ')' in candidate['name']:\n",
    "            target = candidate['id']\n",
    "    return target\n",
    "\n",
    "def get_daily_pageviews(titles, language, start, end):\n",
    "    data = []\n",
    "    for title in tqdm(titles):\n",
    "        \n",
    "        # authors requested we wait 1 second at least in between requests\n",
    "        time.sleep(1)\n",
    "        \n",
    "        wikishark_id = get_wikishark_id(title, language)\n",
    "        \n",
    "        # wait to avoid overloading servers\n",
    "        time.sleep(1)\n",
    "        \n",
    "        response = requests.get(f'https://www.wikishark.com/getdata/daily.php?value={wikishark_id}?view=2&scale=0&normalized=0&loglog=0&log=0&zerofix=0')\n",
    "        daily_data = response.json()\n",
    "        \n",
    "        # add data with timestamps\n",
    "        start_date = datetime.datetime.strptime(start, '%d/%m/%Y')\n",
    "        end_date   = datetime.datetime.strptime(end, '%d/%m/%Y')\n",
    "        current_date = datetime.datetime.now()\n",
    "        \n",
    "        # wikishark returns daily page views for every day since 2007-12-31 (independent of given parameters)\n",
    "        # we need to index it according to the time period we are interested in\n",
    "        start_index = (len(daily_data) - 1) - (current_date - start_date).days\n",
    "        end_index = (len(daily_data) - 1) - (current_date - end_date).days\n",
    "        timestamps = {}\n",
    "        for i, d in enumerate(daily_data[start_index:end_index]):\n",
    "            ts = start_date + datetime.timedelta(days=i)\n",
    "            timestamps[ts] = int(d)\n",
    "    \n",
    "        data.append({**{'Article': title}, **timestamps})\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_en = get_daily_pageviews(list(english_names.head(30).Title), 'en', [2011, 2012, 2013,2014])\n",
    "\n",
    "# use article name as index\n",
    "privacy_en.index = privacy_en.Article\n",
    "privacy_en = privacy_en.drop(['Article'], axis=1)\n",
    "privacy_en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate into monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_en.columns = pd.to_datetime(privacy_en.columns)\n",
    "\n",
    "# take monthly cumulative\n",
    "monthly_en = privacy_en.resample('M', axis=1).sum()\n",
    "monthly_en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Melt dataframe to use dates as entry values rather than columns\n",
    "monthly_en_melt = pd.melt(monthly_en, value_name='views', var_name='date', ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the monthly total pageviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot, dates\n",
    "import datetime\n",
    "\n",
    "reveal = datetime.datetime(2013, 6, 5) #Around june \n",
    "fig, ax = plt.subplots()\n",
    "ax.axvline(reveal,c='r')\n",
    "monthly_en_melt.reset_index().groupby('date').sum().reset_index().plot.scatter(x='date',y='views',ax=ax,rot=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting per article views (total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_en_melt.reset_index().groupby('Article')['views'].sum().sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like most of the traffic generated on these pages comes from the 4Chan article, which has been famous for many scandals unrelated to internet privacy, which means also probably a high variance in views. Since we can safely assume this article to not be helpful regarding the tendency of people to look for privacy-enhancing tools, we blacklist this article from our study. Just to be safe : we check the views per months per article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting per article views (monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6,5,figsize=(20,20),sharey=True)\n",
    "for article,ax in zip(list(monthly_en_melt.index.unique()),axs.flat):\n",
    "    monthly_en_melt.loc[article][['date','views']].reset_index().plot.scatter(x='date',y='views',ax=ax,rot=90)\n",
    "    ax.axvline(reveal,c='r')\n",
    "    ax.set_title(article)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we decide to blacklist 4Chan from our articles. On another note, even at this scale, we see that there seems to be an immediate impact on the pageviews for DuckDuckGo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing 4chan from our articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_en_melt = monthly_en_melt.drop('4chan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A world without 4Chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.axvline(reveal,c='r')\n",
    "monthly_en_melt.reset_index().groupby('date').sum().reset_index().plot.scatter(x='date',y='views',ax=ax,rot=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6,5,figsize=(20,20),sharey=True)\n",
    "for article,ax in zip(list(monthly_en_melt.index.unique()),axs.flat):\n",
    "    monthly_en_melt.loc[article][['date','views']].reset_index().plot.scatter(x='date',y='views',ax=ax,rot=90)\n",
    "    ax.axvline(reveal,c='r')\n",
    "    ax.set_title(article)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_en_melt.reset_index().groupby('Article')['views'].sum().sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
