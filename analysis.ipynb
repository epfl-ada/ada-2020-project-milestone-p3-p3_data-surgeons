{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.core.debugger import set_trace\n",
    "import datetime\n",
    "import requests\n",
    "import time\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_names = pd.read_csv('data/articles.csv', header=None)\n",
    "english_names.columns = ['Title']\n",
    "english_names['Title'] = english_names['Title'].apply(lambda x : x.replace(' ', '_'))\n",
    "english_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qid_from_title(title, language):\n",
    "    response = requests.get(f'https://{language}.wikipedia.org/w/api.php?'\n",
    "                            f'action=query&prop=pageprops&titles={title}&redirects&format=json')\n",
    "    try:\n",
    "        r = [item for item in response.json()['query']['pages'].values()][0]\n",
    "        qid = r['pageprops']['wikibase_item']\n",
    "    except KeyError:\n",
    "        print(f'Article {title} has no Wikidata ID')\n",
    "        return None\n",
    "    return qid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve wikidata entries for each given article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qids = pd.DataFrame([get_qid_from_title(title, 'en') for title in tqdm(english_names.Title.values)])\n",
    "qids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English pageview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_year_to_date(year, days):\n",
    "    return datetime.datetime(year, 1, 1) + datetime.timedelta(days - 1)\n",
    "\n",
    "def get_wikishark_id(article_name, lang):\n",
    "    response = requests.get(f'https://www.wikishark.com/autocomplete.php?q={article_name}')\n",
    "    \n",
    "    r = response.json()\n",
    "    \n",
    "    target = None\n",
    "    for candidate in r:\n",
    "        if '(' + lang + ')' in candidate['name'] and article_name.replace('_',' ').lower() in candidate['name'].lower():\n",
    "            return candidate['id']\n",
    "    return target\n",
    "\n",
    "def get_daily_pageviews(titles, language, start, end):\n",
    "    data = []\n",
    "    for title in tqdm(titles):\n",
    "        \n",
    "        # authors requested we wait 1 second at least in between requests\n",
    "        time.sleep(1)\n",
    "        \n",
    "        wikishark_id = get_wikishark_id(title, language)\n",
    "        if wikishark_id is None:\n",
    "            print(f'Could not find data for {title}')\n",
    "            continue\n",
    "        # wait to avoid overloading servers\n",
    "        time.sleep(1)\n",
    "        \n",
    "        response = requests.get(f'https://www.wikishark.com/getdata/daily.php?value={wikishark_id}?view=2&scale=0&normalized=0&loglog=0&log=0&zerofix=0')\n",
    "\n",
    "        daily_data = response.json()\n",
    "        \n",
    "        # add data with timestamps\n",
    "        start_date = datetime.datetime.strptime(start, '%d/%m/%Y')\n",
    "        end_date   = datetime.datetime.strptime(end, '%d/%m/%Y')\n",
    "        current_date = datetime.datetime.now()\n",
    "        \n",
    "        # wikishark returns daily page views for every day since 2007-12-31 (independent of given parameters)\n",
    "        # we need to index it according to the time period we are interested in\n",
    "        start_index = (len(daily_data) - 1) - (current_date - start_date).days\n",
    "        end_index = (len(daily_data) - 1) - (current_date - end_date).days\n",
    "        timestamps = {}\n",
    "        for i, d in enumerate(daily_data[start_index:end_index]):\n",
    "            ts = start_date + datetime.timedelta(days=i)\n",
    "            timestamps[ts] = int(d)\n",
    "    \n",
    "        data.append({**{'Article': title}, **timestamps})\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PICKLED = './privacy.pkl'\n",
    "\n",
    "if os.path.isfile(PICKLED):\n",
    "    privacy_en = pd.read_pickle(PICKLED)\n",
    "else:\n",
    "    privacy_en = get_daily_pageviews(list(english_names.Title), 'en', '01/01/2011','01/01/2015')\n",
    "    # use article name as index\n",
    "    privacy_en.index = privacy_en.Article\n",
    "    privacy_en = privacy_en.drop(['Article'], axis=1)\n",
    "    privacy_en.to_pickle(PICKLED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate into monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_en.columns = pd.to_datetime(privacy_en.columns)\n",
    "\n",
    "# take monthly cumulative\n",
    "monthly_en = privacy_en.resample('M', axis=1).sum()\n",
    "monthly_en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Melt dataframe to use dates as entry values rather than columns\n",
    "monthly_en_melt = pd.melt(monthly_en, value_name='views', var_name='date', ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardizing per article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    mean = monthly_en_melt.reset_index().groupby('Article').mean()['views'].loc[x['Article']]\n",
    "    std  = monthly_en_melt.reset_index().groupby('Article').std()['views'].loc[x['Article']]\n",
    "    std = std if std != 0 else 1\n",
    "    return (x['views'] - mean)/std\n",
    "\n",
    "monthly_en_melt = monthly_en_melt.reset_index()\n",
    "monthly_en_melt['standardized'] = monthly_en_melt.apply(standardize,axis=1)\n",
    "monthly_en_melt = monthly_en_melt.set_index('Article')\n",
    "monthly_en_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick preprocessing of articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all articles that have a very low amount of views no matter what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_views = [article for article in monthly_en_melt.index.unique() if (monthly_en_melt.loc[article].views < 100).all()]\n",
    "print(low_views)\n",
    "monthly_en_melt = monthly_en_melt.drop(low_views)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all articles that have a too high deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = [article for article in monthly_en_melt.index.unique() if (monthly_en_melt.loc[article].standardized.abs() > 5).any()]\n",
    "print(outliers)\n",
    "monthly_en_melt = monthly_en_melt.drop(outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all articles that were created after the reveal (no views before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_date = '2014-01-01'\n",
    "created_after = [article for article in monthly_en_melt.index.unique() if not (monthly_en_melt.loc[article][['views','date']].set_index('date').loc[:before_date].views > 0).any()]\n",
    "print(created_after)\n",
    "monthly_en_melt = monthly_en_melt.drop(created_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardized vs total views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot, dates\n",
    "import datetime\n",
    "\n",
    "reveal = datetime.datetime(2013, 6, 5) #Around june \n",
    "fig, axs = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "axs[0].axvline(reveal,c='r')\n",
    "axs[1].axvline(reveal,c='r')\n",
    "\n",
    "axs[0].set_title('Standardized')\n",
    "axs[1].set_title('Views')\n",
    "\n",
    "monthly_en_melt.groupby('date').sum().reset_index().plot.scatter(x='date', y='standardized', ax=axs[0], rot=90)\n",
    "monthly_en_melt.groupby('date').sum().reset_index().plot.scatter(x='date', y='views',ax=axs[1],rot=90)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting per article views (total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "monthly_en_melt.reset_index().groupby('Article')['views'].sum().sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like most of the traffic generated on these pages comes from the 4Chan article, which has been famous for many scandals unrelated to internet privacy, which means also probably a high variance in views. Since we can safely assume this article to not be helpful regarding the tendency of people to look for privacy-enhancing tools, we blacklist this article from our study. Just to be safe : we check the views per months per article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting per article views (monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(10,5,figsize=(25,20),sharey=True)\n",
    "for article,ax in zip(list(monthly_en_melt.index.unique()),axs.flat):\n",
    "    monthly_en_melt.loc[article][['date','views']].reset_index().plot.scatter(x='date',y='views',ax=ax,rot=90)\n",
    "    ax.axvline(reveal,c='r')\n",
    "    ax.set_title(article)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we decide to blacklist 4Chan from our articles. On another note, even at this scale, we see that there seems to be an immediate impact on the pageviews for DuckDuckGo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing 4chan from our articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_en_melt = monthly_en_melt.drop('4chan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A world without 4Chan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reveal = datetime.datetime(2013, 6, 5) #Around june \n",
    "fig, axs = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "axs[0].axvline(reveal,c='r')\n",
    "axs[1].axvline(reveal,c='r')\n",
    "\n",
    "axs[0].set_title('Standardized')\n",
    "axs[1].set_title('Views')\n",
    "\n",
    "monthly_en_melt.groupby('date').sum().reset_index().plot.scatter(x='date', y='standardized', ax=axs[0], rot=90)\n",
    "monthly_en_melt.groupby('date').sum().reset_index().plot.scatter(x='date', y='views',ax=axs[1],rot=90)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "monthly_en_melt.reset_index().groupby('Article')['views'].sum().sort_values(ascending=False).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratification / Discretization\n",
    "\n",
    "Given that our distribution of total pageviews per articles can be considered as heavy-tailed, we probably don't want equal width or equal frequency discretization. Instead we opt for clustering to divide our articles into multiple groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MeanShift, KMeans\n",
    "\n",
    "sorted_views = monthly_en_melt.reset_index().groupby('Article').sum().sort_values('views',ascending=False)\n",
    "vals = sorted_views['views'].values.reshape((-1,1))\n",
    "\n",
    "clustering = KMeans(n_clusters=3).fit(vals)\n",
    "#clustering = MeanShift(bandwidth=None).fit(test)\n",
    "\n",
    "plt.scatter(x=range(len(sorted_views)),y=sorted_views['views'], c=clustering.labels_)\n",
    "\n",
    "sorted_views['group'] = clustering.labels_\n",
    "monthly_en_melt['group'] = sorted_views['group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "reveal = datetime.date(2013,6,5)\n",
    "\n",
    "for g in monthly_en_melt.group.unique():\n",
    "    plt.figure()\n",
    "    ax = plt.gca()\n",
    "    df = monthly_en_melt[monthly_en_melt['group'] == g].groupby('date').sum()\n",
    "\n",
    "    before = df.loc[:reveal].reset_index()\n",
    "    after  = df.loc[reveal:].reset_index()\n",
    "    before.date = before.date.astype(int)\n",
    "    after.date = after.date.astype(int)\n",
    "    \n",
    "    sns.regplot(x='date',y='views',ax=ax,data=before)\n",
    "    sns.regplot(x='date',y='views',ax=ax,data=after)\n",
    "    \n",
    "    ax.set_title(f'Group {g}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
