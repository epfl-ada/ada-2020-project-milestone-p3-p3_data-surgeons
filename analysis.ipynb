{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import requests\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.ticker as mticker\n",
    "import matplotlib.dates as mdates\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.cluster import MeanShift, KMeans\n",
    "from IPython.core.debugger import set_trace\n",
    "from matplotlib import pyplot, dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_names = pd.read_csv('data/articles.csv', header=None)\n",
    "english_names.columns = ['Title']\n",
    "english_names['Title'] = english_names['Title'].apply(lambda x : x.replace(' ', '_'))\n",
    "english_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_qid_from_title(title, language):\n",
    "    response = requests.get(f'https://{language}.wikipedia.org/w/api.php?'\n",
    "                            f'action=query&prop=pageprops&titles={title}&redirects&format=json')\n",
    "    try:\n",
    "        r = [item for item in response.json()['query']['pages'].values()][0]\n",
    "        qid = r['pageprops']['wikibase_item']\n",
    "    except KeyError:\n",
    "        print(f'Article {title} has no Wikidata ID')\n",
    "        return None\n",
    "    return qid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve wikidata entries for each given article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qids = pd.DataFrame([get_qid_from_title(title, 'en') for title in tqdm(english_names.Title.values)])\n",
    "qids.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English pageview data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_year_to_date(year, days):\n",
    "    return datetime.datetime(year, 1, 1) + datetime.timedelta(days - 1)\n",
    "\n",
    "def get_wikishark_id(article_name, lang):\n",
    "    response = requests.get(f'https://www.wikishark.com/autocomplete.php?q={article_name}')\n",
    "    \n",
    "    r = response.json()\n",
    "    \n",
    "    target = None\n",
    "    for candidate in r:\n",
    "        if '(' + lang + ')' in candidate['name'] and article_name.replace('_',' ').lower() in candidate['name'].lower():\n",
    "            return candidate['id']\n",
    "    return target\n",
    "\n",
    "def get_daily_pageviews(titles, language, start, end):\n",
    "    data = []\n",
    "    for title in tqdm(titles):\n",
    "        \n",
    "        # authors requested we wait 1 second at least in between requests\n",
    "        time.sleep(1)\n",
    "        \n",
    "        wikishark_id = get_wikishark_id(title, language)\n",
    "        if wikishark_id is None:\n",
    "            print(f'Could not find data for {title}')\n",
    "            continue\n",
    "        # wait to avoid overloading servers\n",
    "        time.sleep(1)\n",
    "        \n",
    "        response = requests.get(f'https://www.wikishark.com/getdata/daily.php?value={wikishark_id}?view=2&scale=0&normalized=0&loglog=0&log=0&zerofix=0')\n",
    "\n",
    "        daily_data = response.json()\n",
    "        \n",
    "        # add data with timestamps\n",
    "        start_date = datetime.datetime.strptime(start, '%d/%m/%Y')\n",
    "        end_date   = datetime.datetime.strptime(end, '%d/%m/%Y')\n",
    "        current_date = datetime.datetime.now()\n",
    "        \n",
    "        # wikishark returns daily page views for every day since 2007-12-31 (independent of given parameters)\n",
    "        # we need to index it according to the time period we are interested in\n",
    "        start_index = (len(daily_data) - 1) - (current_date - start_date).days\n",
    "        end_index = (len(daily_data) - 1) - (current_date - end_date).days\n",
    "        timestamps = {}\n",
    "        for i, d in enumerate(daily_data[start_index:end_index]):\n",
    "            ts = start_date + datetime.timedelta(days=i)\n",
    "            timestamps[ts] = int(d)\n",
    "    \n",
    "        data.append({**{'Article': title}, **timestamps})\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PICKLED = './privacy.pkl'\n",
    "\n",
    "if os.path.isfile(PICKLED):\n",
    "    privacy_en = pd.read_pickle(PICKLED)\n",
    "else:\n",
    "    privacy_en = get_daily_pageviews(list(english_names.Title), 'en', '01/01/2011','01/01/2016')\n",
    "    # use article name as index\n",
    "    privacy_en.index = privacy_en.Article\n",
    "    privacy_en = privacy_en.drop(['Article'], axis=1)\n",
    "    privacy_en.to_pickle(PICKLED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate into monthly data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "privacy_en.columns = pd.to_datetime(privacy_en.columns)\n",
    "\n",
    "# take monthly cumulative\n",
    "monthly_en = privacy_en.resample('M', axis=1).sum()\n",
    "monthly_en.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory data analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Melt dataframe to use dates as entry values rather than columns\n",
    "monthly_en_melt = pd.melt(monthly_en, value_name='views', var_name='date', ignore_index=False)\n",
    "len(monthly_en_melt.index.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick preprocessing of articles\n",
    "\n",
    "Since we have a lot of articles, we want to first remove articles that are of no interest to us, before we can visualize the total dataset correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all articles that have a very low amount of views no matter what"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_views = [article for article in monthly_en_melt.index.unique() if (monthly_en_melt.loc[article].views < 100).all()]\n",
    "print(low_views)\n",
    "monthly_en_melt = monthly_en_melt.drop(low_views)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all articles that may be outliers\n",
    "\n",
    "In the same way the original paper found outlier like the hamas article, we want to remove from our dataset sudden changes in pageviews that are not part of our signal.\n",
    "Since the views an article gets monthly doesn't follow a gaussian distribution, we use IQRs to determine if an article is an outlier rather than just using the 3 standard deviations rule. If the views in a month are higher than the 75th percentile + 3 times the interquartile range, we consider it for manual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_outlier(article, factor=2):\n",
    "    q25, q75 = np.percentile(monthly_en_melt.loc[article].views, 25), np.percentile(monthly_en_melt.loc[article].views, 75)\n",
    "    iqr = q75 - q25\n",
    "    cut_off = iqr * factor\n",
    "    lower, upper = q25 - cut_off, q75 + cut_off\n",
    "    return ((monthly_en_melt.loc[article].views < lower) | (monthly_en_melt.loc[article].views > upper)).any()\n",
    "\n",
    "outliers = [article for article in monthly_en_melt.index.unique() if is_outlier(article)]\n",
    "print(len(outliers),outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manual inspection\n",
    "\n",
    "In order to verify that our potential outliers should be removed, we manually check each of these articles, the news related to it, to find out if there is an obvious reason for the sudden change in views, and if this reason is related to our signal : privacy enhancing technologies.\n",
    "\n",
    "The articles we suspect as outliers are :\n",
    "- I2P : Spike happens around the time Silkroad (drug network) moves from tor to I2p\n",
    "- IMule : High variance all around, bump in 2012 probably related to the bump in use of i2p in 2012\n",
    "- Operation onymous : Spikes around the time of operation, an international law enforcement operation targeting darknet markets and other hidden services operating on the Tor network.\n",
    "\n",
    "**TODO : ADD YOURS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_outliers = ['4chan','I2P', 'IMule', 'Operation_Onymous']\n",
    "monthly_en_melt= monthly_en_melt.drop(manual_outliers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finally some Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting per article views (total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "most_views = monthly_en_melt.reset_index().groupby('Article')['views'].sum().sort_values(ascending=False)\n",
    "head = list(most_views.head(5).index) #a surprise tool that will help us later\n",
    "most_views.plot.bar()\n",
    "plt.title(\"Total pageviews per article\")\n",
    "plt.ylabel(\"Total pageviews\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have removed all those outlier articles, we have a look at what is the real distribution of pageviews for articles that should represent privacy enhancing techniques. We remark that there is still a high gap in the pageviews, for example, the Tor article represents a good percentage of the mass of the distribution.\n",
    "\n",
    "It might look like this tail is quite long, but this is actually because there is a decent amount of articles that were created during the time period of our analysis, and thus don't accumulate as much total views."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting total and head 5 articles pageviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reveal = datetime.datetime(2013, 6, 5) #Around june \n",
    "fig, ax = plt.subplots(sharey=True, figsize=(10,5))\n",
    "monthly_en_melt.reset_index().groupby('date').sum().reset_index().plot.scatter(x='date',y='views',ax=ax,label='All articles')\n",
    "monthly_en_melt.loc[head].reset_index().groupby('date').sum().reset_index().plot.scatter(x='date',y='views',ax=ax,c='r',label='Top 5 articles')\n",
    "plt.axvline(reveal,c='r')\n",
    "plt.title(\"Total pageviews for privacy-enhancing technology articles\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems to be a lot of variance even before the reveal date, with a peak at the reveal date rather than an effect due to the reveal. We see that the top 5 articles have a very high effect on the trend.\n",
    "\n",
    "Something interesting to note is that in the long term, there seems to be a reversing trend : the pageviews tend to go down until 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the pageviews of articles with the most weight\n",
    "\n",
    "Since the first 5 articles are responsible for a good portion of the trend of the data, we investigate further on these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=5, figsize=(30,5), sharey=True)\n",
    "for i,ax in enumerate(axs.flatten()):\n",
    "    monthly_en_melt.loc[head[i]].plot.scatter(x='date',y='views',ax=ax)\n",
    "    ax.axvline(reveal,c='r')\n",
    "    ax.set_title(head[i])\n",
    "plt.suptitle('Article pageviews for top 5 articles (with respect to total views)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the DuckDuckGo article, we are able to see quite an important direct impact on the pageviews, starting from the reveal, however, long term it doesn't seem to affect the trend.\n",
    "\n",
    "We also see an impact on the pageviews for Tor, but since the variance is very high for that article, it is hard to have a definitive answer.\n",
    "\n",
    "For the PGP article, there might be a little bit of an impact, but since there was already a trend of increase it is hard to say.\n",
    "\n",
    "For the SOCKS and PeerGuardian article we can't identify any visual trend in views due to the reveal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmented Regression Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "df = monthly_en_melt.groupby('date').sum()\n",
    "\n",
    "#times before and after the reveal\n",
    "before = df.loc[:reveal].reset_index()\n",
    "after  = df.loc[reveal:].reset_index()\n",
    "\n",
    "#set dates to num so regplot is able to produce a result\n",
    "before.date = mdates.date2num(before.date)\n",
    "after.date  = mdates.date2num(after.date)\n",
    "\n",
    "#regplot plots both scatter, and regression fit\n",
    "sns.regplot(x='date',y='views',ax=ax,data=before,label='before')\n",
    "sns.regplot(x='date',y='views',ax=ax,data=after,label='after')\n",
    "\n",
    "#reveal vertical line\n",
    "\n",
    "plt.axvline(reveal,c='r')\n",
    "plt.legend()\n",
    "plt.title(\"Total pageviews for privacy-enhancing technology articles\")\n",
    "\n",
    "#turn numbers back to dates on the axis\n",
    "loc = mdates.AutoDateLocator()\n",
    "ax.xaxis.set_major_locator(loc)\n",
    "ax.xaxis.set_major_formatter(mdates.AutoDateFormatter(loc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remark a sudden impact in the following few months, but the long term trend doesn't seem to be very much impacted by this reveal. It is interesting to compare this to the global wikipedia trend to see how it compares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statsmodel Regression Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with the global wikipedia pageviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison with the same articles in different languages : French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Experiments :\n",
    "\n",
    "We have tried various other ways for dealing with data with a high difference in pageviews, i.e. to not fit our whole hypothesis on the 3 articles that have the most views. The two following experiments didn't lead to significant conclusions but we leave them here for completeness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Experiment : Standardizing per article\n",
    "\n",
    "In this part, we standardize the views of each articles, thus giving the same weights to all articless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x):\n",
    "    mean = monthly_en_melt.reset_index().groupby('Article').mean()['views'].loc[x['Article']]\n",
    "    std  = monthly_en_melt.reset_index().groupby('Article').std()['views'].loc[x['Article']]\n",
    "    std = std if std != 0 else 1\n",
    "    return (x['views'] - mean)/std\n",
    "\n",
    "monthly_en_melt = monthly_en_melt.reset_index()\n",
    "monthly_en_melt['standardized'] = monthly_en_melt.apply(standardize,axis=1)\n",
    "monthly_en_melt = monthly_en_melt.set_index('Article')\n",
    "monthly_en_melt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardized vs total views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, figsize=(10,4))\n",
    "\n",
    "axs[0].axvline(reveal,c='r')\n",
    "axs[1].axvline(reveal,c='r')\n",
    "\n",
    "axs[0].set_title('Standardized')\n",
    "axs[1].set_title('Views')\n",
    "\n",
    "monthly_en_melt.groupby('date').sum().reset_index().plot.scatter(x='date', y='standardized', ax=axs[0], rot=90)\n",
    "monthly_en_melt.groupby('date').sum().reset_index().plot.scatter(x='date', y='views',ax=axs[1],rot=90)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that we have a high amount of articles, even with our quick preprocessing, we have too many articles that have a low amount of views, which makes them inherently have more variance, so it is unfair to give as much weight to these articles as to the other ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Experiment : Stratification\n",
    "\n",
    "Instead of analyzing a single group of articles that have highly different views, we can select groups of articles that have similar viewcounts, and analyze them together.\n",
    "Given that our distribution of total pageviews per articles can be considered as heavy-tailed, we probably don't want equal width or equal frequency discretization. Instead we opt for clustering to divide our articles into multiple groups, by creating clusters based on the total amount of views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_views = monthly_en_melt.reset_index().groupby('Article').sum().sort_values('views',ascending=False)\n",
    "vals = np.log(sorted_views['views'].values.reshape((-1,1))) #Use log scales values because they are too far away"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting number of clusters\n",
    "\n",
    "Since we don't know exactly how many groups we want to define, we will use the silhouette score and the elbow method to help us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sse(features_X, start=2, end=11):\n",
    "    sse = []\n",
    "    for k in range(start, end):\n",
    "        # Assign the labels to the clusters\n",
    "        kmeans = KMeans(n_clusters=k, random_state=10).fit(features_X)\n",
    "        sse.append({\"k\": k, \"sse\": kmeans.inertia_})\n",
    "\n",
    "    sse = pd.DataFrame(sse)\n",
    "    # Plot the data\n",
    "    plt.plot(sse.k, sse.sse)\n",
    "    plt.xlabel(\"K\")\n",
    "    plt.ylabel(\"Sum of Squared Errors\")\n",
    "    \n",
    "plot_sse(vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "silhouettes = []\n",
    "\n",
    "# Try multiple k\n",
    "for k in range(2, 11):\n",
    "    # Cluster the data and assigne the labels\n",
    "    labels = KMeans(n_clusters=k, random_state=10).fit_predict(vals)\n",
    "    # Get the Silhouette score\n",
    "    score = silhouette_score(vals, labels)\n",
    "    silhouettes.append({\"k\": k, \"score\": score})\n",
    "    \n",
    "# Convert to dataframe\n",
    "silhouettes = pd.DataFrame(silhouettes)\n",
    "\n",
    "# Plot the data\n",
    "plt.plot(silhouettes.k, silhouettes.score)\n",
    "plt.xlabel(\"K\")\n",
    "plt.ylabel(\"Silhouette score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that there is now particular elbow, we take the value with the best silhouette score, K = 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLUSTERS = 5\n",
    "\n",
    "clustering = KMeans(n_clusters=NUM_CLUSTERS).fit(vals)\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.scatter(x=range(len(sorted_views)),y=sorted_views['views'], c=clustering.labels_)\n",
    "plt.gca().set_yscale('log')\n",
    "plt.xlabel('Article')\n",
    "plt.ylabel('Views')\n",
    "plt.gca().set_xticks(range(len(sorted_views)))\n",
    "plt.gca().set_xticklabels(sorted_views.index, rotation=90)\n",
    "\n",
    "sorted_views['group'] = clustering.labels_\n",
    "monthly_en_melt['group'] = sorted_views['group']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reveal = datetime.date(2013,6,5)\n",
    "fig, axs = plt.subplots(nrows=NUM_CLUSTERS, figsize=(10,20))\n",
    "\n",
    "for g,ax in zip(range(len(monthly_en_melt.group.unique())),axs.flatten()):\n",
    "    #Group by date, select the group\n",
    "    selected = monthly_en_melt[monthly_en_melt['group'] == g]\n",
    "    df = selected.groupby('date').sum()\n",
    "\n",
    "    before = df.loc[:reveal].reset_index()\n",
    "    after  = df.loc[reveal:].reset_index()\n",
    "    before.date = mdates.date2num(before.date)\n",
    "    after.date  = mdates.date2num(after.date)\n",
    "    \n",
    "    sns.regplot(x='date',y='views',ax=ax,data=before)\n",
    "    sns.regplot(x='date',y='views',ax=ax,data=after)\n",
    "    \n",
    "    loc = mdates.AutoDateLocator()\n",
    "    ax.xaxis.set_major_locator(loc)\n",
    "    ax.xaxis.set_major_formatter(mdates.AutoDateFormatter(loc))\n",
    "    \n",
    "    sample = selected.reset_index().groupby('Article').sum().sort_values('views', ascending=False).head(3).index\n",
    "    sample_str = ','.join(sample)\n",
    "    ax.set_title(f'Group {g} ({sample_str})')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, we are not very comfortable on interpreting these results, since they show many different trends, so we would have to go through each individual article and verify what causes the trend, so we also stopped with this analysis. Another issue we thought of is that the clusters we would find on other languages of wikipedia would be different so it would be hard to compare a general trend given that we find no general trend here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
